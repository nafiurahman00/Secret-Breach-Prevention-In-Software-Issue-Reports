{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1707329129042,
     "user": {
      "displayName": "Sadif Ahmed",
      "userId": "17613161859460678714"
     },
     "user_tz": -360
    },
    "id": "SPs0eg7jqSVN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import uuid\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "        input_string =  text   \n",
    "        input_string = re.sub(r'[\\'\"\\│]', '', input_string)\n",
    "        dir_list_clean = re.sub(r'drwx[-\\s]*\\d+\\s+\\w+\\s+\\w+\\s+\\d+\\s+\\w+\\s+\\d+\\s+[0-9a-fA-F-]+.*','',input_string)\n",
    "        shell_code_free_text = re.sub(r'```shell([^`]+)```','',dir_list_clean,flags=re.IGNORECASE)\n",
    "        shell_code_free_text = re.sub(r'```Shell\\s*\"([^\"]*)\"\\s*```','',shell_code_free_text,flags=re.IGNORECASE)\n",
    "        # saved_game_free_text = re.sub(r'```([^`]+)```','',shell_code_free_text) #etay jhamela hobe\n",
    "        saved_game_free_text = re.sub(r'<details><summary>Saved game</summary>\\n\\n```(.*?)```', '', shell_code_free_text)\n",
    "        remove_packages = re.sub(r'(\\w+\\.)+\\w+','',saved_game_free_text)\n",
    "        java_exp_free_text = re.sub(r'at\\s[\\w.$]+\\.([\\w]+)\\(([^:]+:\\d+)\\)','',remove_packages)\n",
    "        # url_free_text= re.sub(https?://[^\\s#]+#[A-Za-z0-9\\-]+,'', java_exp_free_text, flags=re.IGNORECASE)\n",
    "        #url_with_fragment_text= re.sub(r'https?://[^\\s#]+#[A-Za-z0-9\\-\\=\\+]+','', java_exp_free_text, flags=re.IGNORECASE)\n",
    "        # url_free_text= re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '',url_with_fragment_text)\n",
    "        commit_free_text= re.sub(r'commit[ ]?(?:id)?[ ]?[:]?[ ]?([0-9a-f]{40})\\b', '', java_exp_free_text, flags=re.IGNORECASE)\n",
    "        file_path_free_text = re.sub(r\"/[\\w/. :-]+\",'',commit_free_text)\n",
    "        file_path_free_text = re.sub( r'(/[^/\\s]+)+','',file_path_free_text)\n",
    "        sha256_free_text = re.sub(r'sha256\\s*[:]?[=]?\\s*[a-fA-F0-9]{64}','',file_path_free_text)\n",
    "        sha1_free_text = re.sub(r'git-tree-sha1\\s*=\\s*[a-fA-F0-9]+','',sha256_free_text)\n",
    "        build_id_free_text = re.sub(r'build-id\\s*[:]?[=]?\\s*([a-fA-F0-9]+)','',sha1_free_text)\n",
    "        guids_free_text = re.sub(r'GUIDs:\\s+([0-9a-fA-F-]+\\s+[0-9a-fA-F-]+\\s+[0-9a-fA-F-]+)','',build_id_free_text)\n",
    "        uuids_free_text = re.sub(r'([0-9a-fA-F-]+\\s*,\\s*[0-9a-fA-F-]+\\s*,\\s*[0-9a-fA-F-]+)','',guids_free_text)\n",
    "        event_id_free_text = re.sub(r'<([^>]+)>','',uuids_free_text)\n",
    "        UUID_free_text = re.sub(r'(?:UUID|GUID|version|id)[\\\\=:\"\\'\\s]*\\b[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}\\b'\n",
    ",'',event_id_free_text,flags=re.IGNORECASE) ##without the prefix so many false positives can be omitted\n",
    "        hex_free_text = re.sub(r'(?:data|address|id)[\\\\=:\"\\'\\s]*\\b0x[0-9a-fA-F]+\\b','',UUID_free_text,flags=re.IGNORECASE) ## deleting hex ids directly can cause issues\n",
    "        ss_free_text = re.sub(r'Screenshot_(\\d{4}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\w+)','',hex_free_text,flags=re.IGNORECASE)\n",
    "        cleaned_text = ss_free_text\n",
    "        return cleaned_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"../dataset/nlbse-2023-issue-report.csv\")\n",
    "# df.shape\n",
    "# df = df[df['labels']=='bug']\n",
    "# print(df.shape)\n",
    "# df_unique = df.drop_duplicates(subset='id', keep='first')\n",
    "# print(df_unique.shape)\n",
    "# df = df_unique\n",
    "# df[\"id\"] = df[\"id\"].astype(int)\n",
    "# df_selected = df[['id','body']]\n",
    "\n",
    "# concatenated_df = pd.read_csv(\"../dataset/Manual-labelled-data-25K.csv\")\n",
    "# concatenated_df_selected = concatenated_df[['Issue_id','Candidate_String', 'is_secret_human_label']]\n",
    "\n",
    "# # Rename columns for consistency\n",
    "# df_selected = df_selected.rename(columns={'body': 'text'})\n",
    "# concatenated_df_selected = concatenated_df_selected.rename(columns={'Candidate_String': 'candidate_string', 'is_secret_human_label': 'label'})\n",
    "\n",
    "# # Merge the two DataFrames on a common key, for example, 'id'\n",
    "# # Adjust the merge key as needed based on your data\n",
    "# merged_df = concatenated_df_selected.merge(df_selected, left_on='Issue_id', right_on='id')\n",
    "# print(merged_df.shape)\n",
    "# columns_to_remove = ['id']\n",
    "# merged_df.drop(columns=columns_to_remove, inplace=True)\n",
    "# print(merged_df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def build_preprocessed_df(df, text_col=\"text\", substr_col=\"candidate_string\"):\n",
    "    \"\"\"\n",
    "    Keep rows where substr_col is a substring of text_col.\n",
    "    Also return counts of matches and mismatches.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        text_col (str): Column with the main string\n",
    "        substr_col (str): Column with candidate substrings\n",
    "\n",
    "    Returns:\n",
    "        preprocessed_df (pd.DataFrame): Rows where substring is found in text\n",
    "        stats (dict): {\"Ok\": match_count, \"Mismatch\": mismatch_count}\n",
    "    \"\"\"\n",
    "    preprocessed_df = None\n",
    "    count = 0\n",
    "    inverse_count = 0\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        main_string = df[text_col].iloc[i]\n",
    "        substring = df[substr_col].iloc[i]\n",
    "\n",
    "        if substring not in main_string:\n",
    "            count += 1\n",
    "        else:\n",
    "            inverse_count += 1\n",
    "            # Append row to preprocessed_df\n",
    "            row_df = pd.DataFrame(df.iloc[i, :]).transpose()\n",
    "            if preprocessed_df is None:\n",
    "                preprocessed_df = row_df\n",
    "            else:\n",
    "                preprocessed_df = pd.concat([preprocessed_df, row_df], ignore_index=True)\n",
    "\n",
    "    if preprocessed_df is None:\n",
    "        preprocessed_df = pd.DataFrame(columns=df.columns)  # empty case\n",
    "\n",
    "    print(f\"Mismatch: {count}\")\n",
    "    print(f\"Ok: {inverse_count}\")\n",
    "    print(preprocessed_df.shape)\n",
    "\n",
    "    return preprocessed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_substrings(df, text_col=\"text\", substr_col=\"candidate_string\"):\n",
    "    \"\"\"\n",
    "    Check if each substring in `substr_col` exists in the corresponding row of `text_col`.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        text_col (str): Name of the column containing main text.\n",
    "        substr_col (str): Name of the column containing candidate substring.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with counts of matches and mismatches.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    inverse_count = 0\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        main_string = df[text_col].iloc[i]\n",
    "        substring = df[substr_col].iloc[i]\n",
    "        if substring in main_string:\n",
    "            count += 1\n",
    "        else:\n",
    "            inverse_count += 1\n",
    "    \n",
    "    print(f\"Mismatch: {inverse_count}\")\n",
    "    print(f\"Ok: {count}\")\n",
    "    \n",
    "    return {\"ok\": count, \"mismatch\": inverse_count}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_substrings(merged_df)\n",
    "# # merged_df[\"text\"] = merged_df[\"text\"].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count=0\n",
    "# inverse_count=0\n",
    "# #print(preprocessed_df['modified_text'][2])\n",
    "# for i in range(merged_df.shape[0]):\n",
    "#   #print(i)\n",
    "#   main_string=merged_df['text'][i]\n",
    "#   substring=merged_df['candidate_string'][i]\n",
    "#   if substring in main_string:\n",
    "#     count+=1\n",
    "#   else:\n",
    "#     inverse_count+=1\n",
    "#     merged_df['text'][i] = preprocess(merged_df[\"text\"][i])\n",
    "# print(\"Mismatch: \"+str(inverse_count))\n",
    "# print(\"Ok: \"+str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_substrings(merged_df)\n",
    "# preprocessed_df = build_preprocessed_df(merged_df)\n",
    "# preprocessed_df.to_csv(\"dataset/og_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_count_labels(file_path: str, label_column: str = 'label'):\n",
    "    \"\"\"\n",
    "    Load a dataframe from a CSV file and count the number of 0s and 1s in the label field.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        label_column (str): Name of the label column. Defaults to 'label'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with counts of 0s and 1s.\n",
    "    \"\"\"\n",
    "    # Load the DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ensure label column exists\n",
    "    if label_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{label_column}' not found in the dataframe.\")\n",
    "    \n",
    "    # Count 0s and 1s\n",
    "    counts = df[label_column].value_counts().to_dict()\n",
    "    \n",
    "    # Report results\n",
    "    zero_count = counts.get(0, 0)\n",
    "    one_count = counts.get(1, 0)\n",
    "    print(f\"Label counts - 0s: {zero_count}, 1s: {one_count}\")\n",
    "    \n",
    "    return {'0': zero_count, '1': one_count}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_0 = preprocessed_df[preprocessed_df[\"label\"] == 0].reset_index(drop=True)\n",
    "# df_1 = preprocessed_df[preprocessed_df[\"label\"] == 1].reset_index(drop=True)\n",
    "# df_1.to_csv(\"dataset/og_pos.csv\")\n",
    "# df_0.to_csv(\"dataset/og_neg.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_count_labels(\"dataset/og_pos.csv\")\n",
    "load_and_count_labels(\"dataset/og_neg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "df1 = pd.read_csv(\"dataset/compiled_tp.csv\")\n",
    "df2 = pd.read_csv(\"dataset/compiled_tp_2.csv\")\n",
    "df3 = pd.read_csv(\"dataset/compiled_tp_3.csv\")\n",
    "df4 = pd.read_csv(\"dataset/compiled_tp_4.csv\")\n",
    "df5 = pd.read_csv(\"dataset/compiled_tp_5.csv\")\n",
    "df_combined = pd.concat([df1, df2,df3,df4,df5], ignore_index=True)\n",
    "df_combined = df_combined[['candidate_string','body']]\n",
    "df_combined = df_combined.rename(columns={'body': 'text'})\n",
    "df_combined[\"Issue_id\"] = [uuid.uuid4() for _ in range(len(df_combined))]\n",
    "df_combined[\"label\"] = 1\n",
    "df_combined.head()\n",
    "df_combined[\"candidate_string\"] = df_combined[\"candidate_string\"].astype(str) \n",
    "df_combined[\"text\"] = df_combined[\"text\"].astype(str) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_substrings(df_combined)\n",
    "count=0\n",
    "inverse_count=0\n",
    "#print(preprocessed_df['modified_text'][2])\n",
    "for i in range(df_combined.shape[0]):\n",
    "  #print(i)\n",
    "  main_string=df_combined['text'][i]\n",
    "  substring=df_combined['candidate_string'][i]\n",
    "  if substring in main_string:\n",
    "    count+=1\n",
    "  else:\n",
    "    inverse_count+=1\n",
    "    df_combined['text'][i] = preprocess(df_combined[\"text\"][i])\n",
    "print(\"Mismatch: \"+str(inverse_count))\n",
    "print(\"Ok: \"+str(count))\n",
    "check_substrings(df_combined)\n",
    "preprocessed_df = build_preprocessed_df(df_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df.to_csv(\"dataset/new_pos.csv\")\n",
    "#preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "df1 = pd.read_csv(\"dataset/compiled_fp.csv\")\n",
    "df2 = pd.read_csv(\"dataset/compiled_fp_2.csv\")\n",
    "df3 = pd.read_csv(\"dataset/compiled_fp_3.csv\")\n",
    "df_combined = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "df_combined = df_combined[['candidate_string','body']]\n",
    "df_combined = df_combined.rename(columns={'body': 'text'})\n",
    "df_combined[\"Issue_id\"] = [uuid.uuid4() for _ in range(len(df_combined))]\n",
    "df_combined[\"label\"] = 0\n",
    "df_combined.head()\n",
    "df_combined[\"candidate_string\"] = df_combined[\"candidate_string\"].astype(str) \n",
    "df_combined[\"text\"] = df_combined[\"text\"].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_substrings(df_combined)\n",
    "count=0\n",
    "inverse_count=0\n",
    "#print(preprocessed_df['modified_text'][2])\n",
    "for i in range(df_combined.shape[0]):\n",
    "  #print(i)\n",
    "  main_string=df_combined['text'][i]\n",
    "  substring=df_combined['candidate_string'][i]\n",
    "  if substring in main_string:\n",
    "    count+=1\n",
    "  else:\n",
    "    inverse_count+=1\n",
    "    df_combined['text'][i] = preprocess(df_combined[\"text\"][i])\n",
    "print(\"Mismatch: \"+str(inverse_count))\n",
    "print(\"Ok: \"+str(count))\n",
    "check_substrings(df_combined)\n",
    "preprocessed_df = build_preprocessed_df(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df.to_csv(\"dataset/new_neg.csv\")\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_count_labels(\"dataset/og_neg.csv\")\n",
    "load_and_count_labels(\"dataset/og_pos.csv\")\n",
    "load_and_count_labels(\"dataset/new_neg.csv\")\n",
    "load_and_count_labels(\"dataset/new_pos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"dataset/og_neg.csv\")\n",
    "df2 = pd.read_csv(\"dataset/new_neg.csv\")\n",
    "#df_neg = pd.concat([df1, df2], ignore_index=True)\n",
    "df_neg = df2\n",
    "\n",
    "df1 = pd.read_csv(\"dataset/og_pos.csv\")\n",
    "df2 = pd.read_csv(\"dataset/new_pos.csv\")\n",
    "#df_pos = pd.concat([df1, df2], ignore_index=True)\n",
    "df_pos = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_neg1,df_neg2= train_test_split(\n",
    "#     df_neg,\n",
    "#     test_size=0.5,\n",
    "#     random_state=42,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_val_df,pos_test_df= train_test_split(\n",
    "    df_pos,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    ")\n",
    "pos_train_df,pos_val_df= train_test_split(\n",
    "    pos_train_val_df,\n",
    "    test_size=0.118,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "neg_train_val_df,neg_test_df= train_test_split(\n",
    "    df_neg,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    ")\n",
    "neg_train_df,neg_val_df= train_test_split(\n",
    "    neg_train_val_df,\n",
    "    test_size=0.118,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([pos_train_df, neg_train_df], ignore_index=True)\n",
    "df_val = pd.concat([pos_val_df, neg_val_df], ignore_index=True)\n",
    "df_test = pd.concat([pos_test_df, neg_test_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"Issue_id\"] = [uuid.uuid4() for _ in range(len(df_train))]\n",
    "df_val[\"Issue_id\"] = [uuid.uuid4() for _ in range(len(df_val))]\n",
    "df_test[\"Issue_id\"] = [uuid.uuid4() for _ in range(len(df_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_dataframe(input_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Prepare X_text, X_candidate, Y_labels from a raw dataframe.\n",
    "    - Cleans NaN/±inf\n",
    "    - Casts text/candidate to string\n",
    "    - Casts label to int (dropping NaN labels)\n",
    "    - Builds clean_text and modified_text columns\n",
    "    Returns: (X_text, X_candidate, Y_labels)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) Validate input ---\n",
    "    if not isinstance(input_df, pd.DataFrame):\n",
    "        raise TypeError(\"Input must be a Pandas DataFrame.\")\n",
    "    required_cols = {\"Issue_id\",\"text\", \"candidate_string\", \"label\"}\n",
    "    missing = required_cols - set(input_df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required column(s): {missing}\")\n",
    "\n",
    "    if input_df.empty:\n",
    "        print(\"Warning: Input DataFrame is empty.\")\n",
    "        return [], [], []\n",
    "\n",
    "    # --- 2) Work on a copy ---\n",
    "    df = input_df.copy()\n",
    "\n",
    "    # --- 3) Clean candidate_string & text: replace inf, drop NaN if you need strict non-empty ---\n",
    "    for col in [\"candidate_string\", \"text\"]:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "        # If you prefer dropping rows where these are missing, uncomment next line:\n",
    "        df = df.dropna(subset=[col])\n",
    "        # Otherwise, keep rows but fill with empty string to avoid tokenizer errors:\n",
    "        #df[col] = df[col].astype(\"string\").fillna(\"\")\n",
    "\n",
    "    # --- 4) Clean label: replace inf→NaN, drop NaN, cast to int ---\n",
    "    df[\"label\"] = df[\"label\"].replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.dropna(subset=[\"label\"])\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= process_dataframe(df_train)\n",
    "df_val= process_dataframe(df_val)\n",
    "df_test= process_dataframe(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_val = df_val.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"dataset/train.csv\")\n",
    "df_val.to_csv(\"dataset/val.csv\")\n",
    "df_test.to_csv(\"dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_and_count_labels(\"dataset/train.csv\")\n",
    "load_and_count_labels(\"dataset/val.csv\")\n",
    "load_and_count_labels(\"dataset/test.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
