{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f85f0cdd",
   "metadata": {},
   "source": [
    "# Security String Classification Training with Unsloth\n",
    "\n",
    "This notebook demonstrates step-by-step training of a security classification model using Unsloth and Llama 3.1. The model classifies strings as either \"Secret\" or \"Non-sensitive\" based on their context in issue reports.\n",
    "\n",
    "## Overview\n",
    "- **Task**: Binary classification of security-sensitive strings\n",
    "- **Model**: Llama 3.1 8B with LoRA fine-tuning\n",
    "- **Framework**: Unsloth for efficient training\n",
    "- **Data**: CSV format with candidate strings and issue reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5079ce07",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries and set up the environment for Unsloth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a9754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set environment for Unsloth\n",
    "os.environ[\"UNSLOTH_IS_PRESENT\"] = \"1\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "print(\"✅ Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48a0eb",
   "metadata": {},
   "source": [
    "## Step 2: Data Loading and Exploration\n",
    "\n",
    "Load the training, validation, and test datasets and explore their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3fb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "df_train = pd.read_csv(\"../Data/train.csv\")\n",
    "df_val = pd.read_csv(\"../Data/val.csv\")\n",
    "df_test = pd.read_csv(\"../Data/test.csv\")\n",
    "\n",
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Val shape: {df_val.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")\n",
    "\n",
    "# Display data structure\n",
    "print(\"\\n📊 Training data sample:\")\n",
    "print(df_train.head(2))\n",
    "\n",
    "print(\"\\n📋 Column information:\")\n",
    "print(df_train.columns.tolist())\n",
    "print(f\"\\nData types:\\n{df_train.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2aa2cb",
   "metadata": {},
   "source": [
    "## Step 3: Model Configuration\n",
    "\n",
    "Configure the Llama 3.1 model with optimal settings for security classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e321453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Model Configuration (following finetune_balanced.py style)\n",
    "# ============================================\n",
    "max_seq_length = 1024  # Context window size\n",
    "dtype = None  # Auto-detect optimal dtype\n",
    "load_in_4bit = True  # Use 4-bit quantization for memory efficiency\n",
    "\n",
    "print(\"🔧 Model Configuration:\")\n",
    "print(f\"Max sequence length: {max_seq_length}\")\n",
    "print(f\"4-bit quantization: {load_in_4bit}\")\n",
    "\n",
    "# Load pre-quantized Llama 3.1 model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"✅ Base model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae2776e",
   "metadata": {},
   "source": [
    "## Step 4: LoRA Configuration\n",
    "\n",
    "Add LoRA (Low-Rank Adaptation) layers for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d9fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters for efficient fine-tuning\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank - balance between efficiency and performance\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,  # LoRA scaling parameter\n",
    "    lora_dropout=0,  # No dropout for stability\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized checkpointing\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(\"✅ LoRA adapters added successfully!\")\n",
    "print(f\"📊 LoRA Configuration:\")\n",
    "print(f\"  - Rank (r): 16\")\n",
    "print(f\"  - Alpha: 16\") \n",
    "print(f\"  - Dropout: 0\")\n",
    "print(f\"  - Target modules: 7 attention/MLP layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a388a30",
   "metadata": {},
   "source": [
    "## Step 5: Data Preprocessing\n",
    "\n",
    "Preprocess the data to create context windows and format labels appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be401933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Apply text preprocessing to clean the input\"\"\"\n",
    "    input_string = text   \n",
    "    input_string = re.sub(r'[\\'\"\\│]', '', input_string)\n",
    "    dir_list_clean = re.sub(r'drwx[-\\s]*\\d+\\s+\\w+\\s+\\w+\\s+\\d+\\s+\\w+\\s+\\d+\\s+[0-9a-fA-F-]+.*','',input_string)\n",
    "    shell_code_free_text = re.sub(r'```shell([^`]+)```','',dir_list_clean,flags=re.IGNORECASE)\n",
    "    shell_code_free_text = re.sub(r'```Shell\\s*\"([^\"]*)\"\\s*```','',shell_code_free_text,flags=re.IGNORECASE)\n",
    "    saved_game_free_text = re.sub(r'<details><summary>Saved game</summary>\\n\\n```(.*?)```', '', shell_code_free_text)\n",
    "    remove_packages = re.sub(r'(\\w+\\.)+\\w+','',saved_game_free_text)\n",
    "    java_exp_free_text = re.sub(r'at\\s[\\w.$]+\\.([\\w]+)\\(([^:]+:\\d+)\\)','',remove_packages)\n",
    "    url_with_fragment_text = re.sub(r'https?://[^\\s#]+#[A-Za-z0-9\\-\\=\\+]+','', java_exp_free_text, flags=re.IGNORECASE)\n",
    "    url_free_text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '',url_with_fragment_text)\n",
    "    commit_free_text = re.sub(r'commit[ ]?(?:id)?[ ]?[:]?[ ]?([0-9a-f]{40})\\b', '', url_free_text, flags=re.IGNORECASE)\n",
    "    file_path_free_text = re.sub(r\"/[\\w/. :-]+\",'',commit_free_text)\n",
    "    file_path_free_text = re.sub(r'(/[^/\\s]+)+','',file_path_free_text)\n",
    "    sha256_free_text = re.sub(r'sha256\\s*[:]?[=]?\\s*[a-fA-F0-9]{64}','',file_path_free_text)\n",
    "    sha1_free_text = re.sub(r'git-tree-sha1\\s*=\\s*[a-fA-F0-9]+','',sha256_free_text)\n",
    "    build_id_free_text = re.sub(r'build-id\\s*[:]?[=]?\\s*([a-fA-F0-9]+)','',sha1_free_text)\n",
    "    guids_free_text = re.sub(r'GUIDs:\\s+([0-9a-fA-F-]+\\s+[0-9a-fA-F-]+\\s+[0-9a-fA-F-]+)','',build_id_free_text)\n",
    "    uuids_free_text = re.sub(r'([0-9a-fA-F-]+\\s*,\\s*[0-9a-fA-F-]+\\s*,\\s*[0-9a-fA-F-]+)','',guids_free_text)\n",
    "    event_id_free_text = re.sub(r'<([^>]+)>','',uuids_free_text)\n",
    "    UUID_free_text = re.sub(r'(?:UUID|GUID|version|id)[\\\\=:\"\\'\\s]*\\b[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}\\b','',event_id_free_text,flags=re.IGNORECASE)\n",
    "    hex_free_text = re.sub(r'(?:data|address|id)[\\\\=:\"\\'\\s]*\\b0x[0-9a-fA-F]+\\b','',UUID_free_text,flags=re.IGNORECASE)\n",
    "    ss_free_text = re.sub(r'Screenshot_(\\d{4}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\w+)','',hex_free_text,flags=re.IGNORECASE)\n",
    "    cleaned_text = ss_free_text\n",
    "    return cleaned_text\n",
    "\n",
    "def create_context_window(text, target_string, window_size=200):\n",
    "    \"\"\"Create context window around target string\"\"\"\n",
    "    target_index = text.find(target_string)\n",
    "    if target_index != -1:\n",
    "        start_index = max(0, target_index - window_size)\n",
    "        end_index = min(len(text), target_index + len(target_string) + window_size)\n",
    "        context_window = text[start_index:end_index]\n",
    "        return context_window\n",
    "    return None\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"🔄 Preprocessing data...\")\n",
    "df_train['modified_text'] = df_train.apply(lambda row: create_context_window(row['text'], row['candidate_string']), axis=1)\n",
    "df_val['modified_text'] = df_val.apply(lambda row: create_context_window(row['text'], row['candidate_string']), axis=1)\n",
    "df_test['modified_text'] = df_test.apply(lambda row: create_context_window(row['text'], row['candidate_string']), axis=1)\n",
    "\n",
    "# Convert labels to text format\n",
    "df_train['label'] = df_train['label'].replace({0: 'Non-sensitive', 1: 'Secret'})\n",
    "df_val['label'] = df_val['label'].replace({0: 'Non-sensitive', 1: 'Secret'})\n",
    "df_test['label'] = df_test['label'].replace({0: 'Non-sensitive', 1: 'Secret'})\n",
    "\n",
    "print(\"✅ Data preprocessing complete!\")\n",
    "print(f\"📊 Label distribution in training data:\")\n",
    "print(df_train['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb4821f",
   "metadata": {},
   "source": [
    "## Step 6: Prompt Template Design\n",
    "\n",
    "Create the prompt template using ChatML format for consistent training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec81ea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(candidate_string, issue_report, label=None):\n",
    "    \"\"\"Format the training prompt using ChatML format\"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a security auditor or classifier specialized in identifying and categorizing sensitive secrets from issue reports. Classify the given candidate string as either \"Non-sensitive\" or \"Secret\" based on its context.\n",
    "\n",
    "A \"Secret\" includes sensitive information such as: \n",
    "- API keys and secrets (e.g., `sk_test_ABC123`)  \n",
    "- Private and secret keys (e.g., private SSH keys, private cryptographic keys)  \n",
    "- Authentication keys and tokens (e.g., `Bearer <token>`)  \n",
    "- Database connection strings with credentials (e.g., `mongodb://user:password@host:port`)  \n",
    "- Passwords, usernames, and any other private information that should not be shared openly.  \n",
    "\n",
    "A \"Non-sensitive\" string is not considered secret and can be shared openly. This includes:  \n",
    "- Public keys of any form (e.g., public SSH keys)  \n",
    "- Non-sensitive configuration values or identifiers  \n",
    "- Actual-looking keys that are clearly marked as dummy/test (e.g., with comments like '# dummy key' or variable names like 'test_key')  \n",
    "- Strings that just look random or patterned but are not actually secrets (e.g., `xyz123`, 'xxxx', `abc123`, `EXAMPLE_KEY`, `token_value`)  \n",
    "- Strings that are clearly placeholders or redacted text (e.g., 'XXXXXXXX', '[REDACTED]', '[TRUNCATED]')  \n",
    "- **Obfuscated or masked values (e.g., '****', '****123', 'abc...xyz')**  \n",
    "\n",
    "These are always considered **\"Non-sensitive\"**, even if they appear in a sensitive context.\n",
    "\n",
    "Reply with only the classification: \"Non-sensitive\" or \"Secret\".\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Classify the given candidate string based on its role in the provided issue report.\n",
    "\n",
    "candidate_string: {candidate_string}\n",
    "issue_report: {issue_report}\"\"\"\n",
    "\n",
    "    if label is not None:\n",
    "        # Training format\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "{system_prompt}<|im_end|>\n",
    "<|im_start|>user\n",
    "{user_prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{label}<|im_end|>\"\"\"\n",
    "    else:\n",
    "        # Inference format\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "{system_prompt}<|im_end|>\n",
    "<|im_start|>user\n",
    "{user_prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format examples for training using the ChatML style\"\"\"\n",
    "    candidate_strings = examples[\"candidate_string\"]\n",
    "    issue_reports = examples[\"modified_text\"]\n",
    "    labels = examples[\"label\"]\n",
    "    texts = []\n",
    "    \n",
    "    for candidate_string, issue_report, label in zip(candidate_strings, issue_reports, labels):\n",
    "        text = format_prompt(candidate_string, issue_report, label)\n",
    "        texts.append(text)\n",
    "    \n",
    "    return texts\n",
    "\n",
    "# Test the prompt formatting\n",
    "sample_row = df_train.iloc[0]\n",
    "sample_prompt = format_prompt(\n",
    "    sample_row['candidate_string'], \n",
    "    sample_row['modified_text'][:200] + \"...\", \n",
    "    sample_row['label']\n",
    ")\n",
    "\n",
    "print(\"✅ Prompt template created!\")\n",
    "print(\"\\n📝 Sample formatted prompt:\")\n",
    "print(sample_prompt[:500] + \"...\" if len(sample_prompt) > 500 else sample_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a74ebed",
   "metadata": {},
   "source": [
    "## Step 7: Dataset Preparation\n",
    "\n",
    "Prepare datasets for training using HuggingFace's Dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc8c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "X_train = df_train\n",
    "X_eval = df_val\n",
    "X_test = df_test\n",
    "y_true = X_test['label']\n",
    "\n",
    "# Convert to HuggingFace datasets\n",
    "train_dataset = Dataset.from_pandas(X_train[[\"candidate_string\", \"modified_text\", \"label\"]])\n",
    "eval_dataset = Dataset.from_pandas(X_eval[[\"candidate_string\", \"modified_text\", \"label\"]])\n",
    "\n",
    "print(\"✅ Datasets prepared!\")\n",
    "print(f\"📊 Dataset sizes:\")\n",
    "print(f\"  - Training: {len(train_dataset):,} examples\")\n",
    "print(f\"  - Validation: {len(eval_dataset):,} examples\")\n",
    "print(f\"  - Test: {len(X_test):,} examples\")\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"\\n📋 Training dataset features: {train_dataset.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403f84c2",
   "metadata": {},
   "source": [
    "## Step 8: Training Configuration\n",
    "\n",
    "Set up the training arguments and SFT trainer with optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8658d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Training Configuration (Optimized for Lower VRAM Usage)\n",
    "# ============================================\n",
    "output_dir = \"../models/llama-3.1-ft-unsloth\"\n",
    "\n",
    "# Initialize trainer with memory-optimized arguments\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,  # Reduced from 4 to 1\n",
    "        per_device_eval_batch_size=1,   # Reduced from 2 to 1\n",
    "        gradient_accumulation_steps=8,  # Increased from 2 to 8 to maintain effective batch size\n",
    "        warmup_steps=20,\n",
    "        num_train_epochs=5,\n",
    "        max_steps=-1,\n",
    "        learning_rate=1e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=500,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=output_dir,\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        eval_strategy=\"epoch\",\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_drop_last=True,\n",
    "        label_smoothing_factor=0.1,\n",
    "        # Memory optimization settings\n",
    "        dataloader_pin_memory=False,     # Disable pin memory to save VRAM\n",
    "        gradient_checkpointing=True,     # Enable gradient checkpointing\n",
    "        torch_compile=False,             # Disable torch compile for memory savings\n",
    "        ddp_find_unused_parameters=False, # Optimize DDP\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"✅ Memory-optimized training configuration complete!\")\n",
    "print(f\"🎯 Training parameters (VRAM optimized):\")\n",
    "print(f\"  - Batch size: 1 (per device) - REDUCED for memory saving\")\n",
    "print(f\"  - Gradient accumulation: 8 steps - INCREASED to maintain effective batch size\")\n",
    "print(f\"  - Effective batch size: 1 × 8 = 8\")\n",
    "print(f\"  - Learning rate: 1e-4\")\n",
    "print(f\"  - Max epochs: 5\")\n",
    "print(f\"  - Optimizer: AdamW 8-bit\")\n",
    "print(f\"  - Save strategy: epoch\")\n",
    "print(f\"  - Eval strategy: epoch\")\n",
    "print(f\"  - Gradient checkpointing: ENABLED for memory savings\")\n",
    "print(f\"  - Pin memory: DISABLED for VRAM optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd92c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Additional Memory Optimization Techniques\n",
    "# ============================================\n",
    "\n",
    "# Clear any existing cached memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"🧹 Cleared CUDA cache\")\n",
    "\n",
    "# Optional: Further reduce sequence length if still using too much memory\n",
    "# Uncomment the next lines if you need even more memory savings\n",
    "# max_seq_length = 512  # Reduce from 1024 to 512\n",
    "# print(f\"⚠️  Sequence length reduced to {max_seq_length} for memory optimization\")\n",
    "\n",
    "# Monitor current memory usage before training\n",
    "if torch.cuda.is_available():\n",
    "    current_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved_memory = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"📊 Current VRAM usage before training:\")\n",
    "    print(f\"  - Allocated: {current_memory:.2f} GB\")\n",
    "    print(f\"  - Reserved: {reserved_memory:.2f} GB\")\n",
    "    \n",
    "    # Get GPU info\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    total_memory = gpu_props.total_memory / 1024**3\n",
    "    available_memory = total_memory - reserved_memory\n",
    "    print(f\"  - Total GPU memory: {total_memory:.2f} GB\")\n",
    "    print(f\"  - Available memory: {available_memory:.2f} GB\")\n",
    "    \n",
    "    if reserved_memory > 12:  # If using more than 12GB\n",
    "        print(\"⚠️  HIGH MEMORY USAGE DETECTED!\")\n",
    "        print(\"💡 Consider these additional optimizations:\")\n",
    "        print(\"   1. Reduce max_seq_length to 512 or 256\")\n",
    "        print(\"   2. Use DeepSpeed ZeRO if available\")\n",
    "        print(\"   3. Enable more aggressive gradient checkpointing\")\n",
    "        print(\"   4. Consider using 8-bit AdamW optimizer\")\n",
    "\n",
    "print(\"✅ Memory optimization setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf0d97c",
   "metadata": {},
   "source": [
    "## Step 9: Memory Monitoring and Training\n",
    "\n",
    "Monitor GPU memory usage and start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e9058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"🖥️  GPU Information:\")\n",
    "print(f\"  - GPU: {gpu_stats.name}\")\n",
    "print(f\"  - Max memory: {max_memory} GB\")\n",
    "print(f\"  - Reserved before training: {start_gpu_memory} GB\")\n",
    "print(f\"  - Available memory: {max_memory - start_gpu_memory:.1f} GB\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\n🚀 Starting training...\")\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"✅ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d335e77",
   "metadata": {},
   "source": [
    "## Step 10: Training Results and Memory Analysis\n",
    "\n",
    "Analyze training results and memory usage statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86360b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "\n",
    "print(f\"📊 Training Statistics:\")\n",
    "print(f\"  - Training time: {trainer_stats.metrics['train_runtime']} seconds\")\n",
    "print(f\"  - Training time: {round(trainer_stats.metrics['train_runtime']/60, 2)} minutes\")\n",
    "print(f\"  - Peak reserved memory: {used_memory} GB\")\n",
    "print(f\"  - Peak memory for training: {used_memory_for_lora} GB\")\n",
    "print(f\"  - Peak memory % of max: {used_percentage}%\")\n",
    "print(f\"  - Training memory % of max: {lora_percentage}%\")\n",
    "\n",
    "print(f\"\\n🎯 Training Metrics:\")\n",
    "for key, value in trainer_stats.metrics.items():\n",
    "    if key.startswith('train_'):\n",
    "        print(f\"  - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd063bb",
   "metadata": {},
   "source": [
    "## Step 11: Model Saving\n",
    "\n",
    "Save the trained model in multiple formats for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a7957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "print(\"💾 Saving model...\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Save as merged 16bit model for easier inference\n",
    "print(\"💾 Saving merged model...\")\n",
    "model.save_pretrained_merged(output_dir + \"_merged\", tokenizer, save_method=\"merged_16bit\")\n",
    "\n",
    "print(\"✅ Model saving complete!\")\n",
    "print(f\"📁 Models saved to:\")\n",
    "print(f\"  - LoRA adapters: {output_dir}\")\n",
    "print(f\"  - Merged model: {output_dir}_merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e9034",
   "metadata": {},
   "source": [
    "## Step 12: Comprehensive Model Testing\n",
    "\n",
    "Now let's run a comprehensive evaluation on the test set to get detailed performance metrics, similar to the evaluation notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40459e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "def format_prompt_for_inference(candidate_string, issue_report):\n",
    "    \"\"\"Format prompt for inference using ChatML style\"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a security expert classifier. Classify the given candidate string as either \"Non-sensitive\" or \"Secret\" based on its context.\n",
    "\n",
    "A \"Secret\" includes sensitive information such as: \n",
    "- API keys and secrets (e.g., `sk_test_ABC123`)  \n",
    "- Private and secret keys (e.g., private SSH keys, private cryptographic keys)  \n",
    "- Authentication keys and tokens (e.g., `Bearer <token>`)  \n",
    "- Database connection strings with credentials (e.g., `mongodb://user:password@host:port`)  \n",
    "- Passwords, usernames, and any other private information that should not be shared openly.  \n",
    "\n",
    "A \"Non-sensitive\" string is not considered secret and can be shared openly. This includes:  \n",
    "- Public keys of any form (e.g., public SSH keys)  \n",
    "- Non-sensitive configuration values or identifiers  \n",
    "- Actual-looking keys that are clearly marked as dummy/test (e.g., with comments like '# dummy key' or variable names like 'test_key')  \n",
    "- Strings that just look random or patterned but are not actually secrets (e.g., `xyz123`, 'xxxx', `abc123`, `EXAMPLE_KEY`, `token_value`)  \n",
    "- Strings that are clearly placeholders or redacted text (e.g., 'XXXXXXXX', '[REDACTED]', '[TRUNCATED]')  \n",
    "- **Obfuscated or masked values (e.g., '****', '****123', 'abc...xyz')**  \n",
    "\n",
    "These are always considered **\"Non-sensitive\"**, even if they appear in a sensitive context.\n",
    "\n",
    "Reply with only the classification: \"Non-sensitive\" or \"Secret\".\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Classify the given candidate string based on its role in the provided issue report.\n",
    "\n",
    "candidate_string: {candidate_string}\n",
    "issue_report: {issue_report}\"\"\"\n",
    "\n",
    "    # Inference format using ChatML style\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "{system_prompt}<|im_end|>\n",
    "<|im_start|>user\n",
    "{user_prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def extract_label(model_response):\n",
    "    \"\"\"Extract label from model response\"\"\"\n",
    "    if \"Secret\" in model_response:\n",
    "        return \"Secret\"\n",
    "    else:\n",
    "        return \"Non-sensitive\"\n",
    "\n",
    "def predict_single(candidate_string, issue_report, model, tokenizer):\n",
    "    \"\"\"Single prediction function for testing\"\"\"\n",
    "    # Format prompt for inference\n",
    "    test_prompt = format_prompt_for_inference(candidate_string, issue_report)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        test_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length\n",
    "    )\n",
    "    \n",
    "    # Move to GPU\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_new_tokens=5,\n",
    "            temperature=0.1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=False\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    assistant_marker = '<|im_start|>assistant'\n",
    "    if assistant_marker in response:\n",
    "        model_response = response.split(assistant_marker)[-1].strip()\n",
    "    else:\n",
    "        model_response = response[len(test_prompt):].strip()\n",
    "    \n",
    "    predicted_label = extract_label(model_response)\n",
    "    return predicted_label, model_response\n",
    "\n",
    "print(\"✅ Prediction functions defined!\")\n",
    "print(\"  - format_prompt_for_inference: Formats prompts for inference\")\n",
    "print(\"  - extract_label: Extracts classification labels from model responses\")\n",
    "print(\"  - predict_single: Makes single predictions with the trained model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a8f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_batch(test_df, model, tokenizer, batch_size=8):\n",
    "    \"\"\"Batch prediction function for comprehensive testing\"\"\"\n",
    "    y_pred = []\n",
    "    errors = []\n",
    "    \n",
    "    print(f\"🔄 Running batch predictions on {len(test_df):,} examples...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(test_df), batch_size), desc=\"Predicting\"):\n",
    "        batch = test_df.iloc[i:i+batch_size]\n",
    "        \n",
    "        for idx, row in batch.iterrows():\n",
    "            try:\n",
    "                predicted_label = predict_single(\n",
    "                    row[\"candidate_string\"], \n",
    "                    row[\"modified_text\"], \n",
    "                    model, \n",
    "                    tokenizer\n",
    "                )\n",
    "                y_pred.append(predicted_label)\n",
    "                print(predicted_label)\n",
    "            except Exception as e:\n",
    "                errors.append(f\"Error at index {idx}: {e}\")\n",
    "                y_pred.append(\"Non-sensitive\")  # Default prediction\n",
    "                continue\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"⚠️  {len(errors)} errors occurred during prediction:\")\n",
    "        for error in errors[:3]:  # Show first 3 errors\n",
    "            print(f\"  - {error}\")\n",
    "        if len(errors) > 3:\n",
    "            print(f\"  - ... and {len(errors) - 3} more errors\")\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "print(\"✅ Batch prediction function defined!\")\n",
    "\n",
    "# def predict_batch(test_df, model, tokenizer, batch_size=32):\n",
    "#     y_pred, errors = [], []\n",
    "#     print(f\"🔄 Running batch predictions on {len(test_df):,} examples...\")\n",
    "\n",
    "#     for i in tqdm(range(0, len(test_df), batch_size), desc=\"Predicting\"):\n",
    "#         batch = test_df.iloc[i:i+batch_size]\n",
    "#         prompts = [\n",
    "#             format_prompt_for_inference(row[\"candidate_string\"], row[\"modified_text\"])\n",
    "#             for _, row in batch.iterrows()\n",
    "#         ]\n",
    "        \n",
    "#         try:\n",
    "#             # Tokenize as a batch\n",
    "#             inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = model.generate(\n",
    "#                     **inputs,\n",
    "#                     max_new_tokens=5,   # only need short outputs\n",
    "#                     do_sample=False,    # deterministic\n",
    "#                     temperature=0.0,\n",
    "#                     pad_token_id=tokenizer.eos_token_id,\n",
    "#                     eos_token_id=tokenizer.eos_token_id\n",
    "#                 )\n",
    "\n",
    "#             decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "#             for resp in decoded:\n",
    "#                 y_pred.append(extract_label(resp))\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             for idx in batch.index:\n",
    "#                 errors.append(f\"Error at index {idx}: {e}\")\n",
    "#                 y_pred.append(\"Non-sensitive\")\n",
    "\n",
    "#     if errors:\n",
    "#         print(f\"⚠️ {len(errors)} errors occurred during prediction.\")\n",
    "#     return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dd032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive evaluation on test set\n",
    "print(\"🚀 Running comprehensive evaluation on test set...\")\n",
    "\n",
    "# Get predictions for the entire test set\n",
    "y_pred = predict_batch(X_test, model, tokenizer)\n",
    "y_true_test = X_test['label'].tolist()\n",
    "\n",
    "print(f\"✅ Evaluation completed!\")\n",
    "print(f\"📊 Prediction Summary:\")\n",
    "print(f\"  - Total predictions: {len(y_pred):,}\")\n",
    "print(f\"  - Unique predicted labels: {set(y_pred)}\")\n",
    "print(f\"  - True label distribution: {X_test['label'].value_counts().to_dict()}\")\n",
    "\n",
    "# Quick accuracy check\n",
    "correct_predictions = sum(1 for true, pred in zip(y_true_test, y_pred) if true == pred)\n",
    "quick_accuracy = correct_predictions / len(y_pred) if len(y_pred) > 0 else 0\n",
    "print(f\"  - Quick accuracy: {quick_accuracy:.3f} ({correct_predictions}/{len(y_pred)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4d4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Detailed Performance Metrics\n",
    "# ============================================\n",
    "accuracy = 0.0\n",
    "precision_avg = 0.0\n",
    "recall_avg = 0.0\n",
    "f1_avg = 0.0\n",
    "\n",
    "if len(y_pred) > 0:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"📈 DETAILED PERFORMANCE METRICS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"\\n📊 Classification Report:\")\n",
    "    print(classification_report(y_true_test, y_pred))\n",
    "    \n",
    "    # Calculate precision, recall, F1-score for each class\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    labels = sorted(set(y_true_test))\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true_test, \n",
    "        y_pred, \n",
    "        labels=labels\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🏷️  Per-Class Detailed Metrics:\")\n",
    "    for i, label in enumerate(labels):\n",
    "        print(f\"\\n  {label.upper()}:\")\n",
    "        print(f\"    - Precision: {precision[i]:.3f}\")\n",
    "        print(f\"    - Recall: {recall[i]:.3f}\")\n",
    "        print(f\"    - F1-score: {f1[i]:.3f}\")\n",
    "        print(f\"    - Support: {support[i]:,}\")\n",
    "    \n",
    "    # Overall accuracy and binary metrics\n",
    "    def map_func(x):\n",
    "        return 1 if x == \"Secret\" else 0\n",
    "\n",
    "    y_true_mapped = np.array([map_func(label) for label in y_true_test])\n",
    "    y_pred_mapped = np.array([map_func(label) for label in y_pred])\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    overall_accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "    \n",
    "    # Calculate weighted averages for overall metrics\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    precision_overall, recall_overall, f1_overall, _ = precision_recall_fscore_support(\n",
    "        y_true_mapped, y_pred_mapped, average='weighted'\n",
    "    )\n",
    "    \n",
    "    # Store these for later use in performance summary\n",
    "    accuracy = overall_accuracy\n",
    "    precision_avg = precision_overall\n",
    "    recall_avg = recall_overall\n",
    "    f1_avg = f1_overall\n",
    "    \n",
    "    print(f\"\\n🎯 Overall Performance:\")\n",
    "    print(f\"  - Overall Accuracy: {overall_accuracy:.3f}\")\n",
    "    print(f\"  - Weighted Precision: {precision_overall:.3f}\")\n",
    "    print(f\"  - Weighted Recall: {recall_overall:.3f}\")\n",
    "    print(f\"  - Weighted F1-Score: {f1_overall:.3f}\")\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    for label_val, name in zip([0, 1], [\"Non-sensitive\", \"Secret\"]):\n",
    "        label_indices = np.where(y_true_mapped == label_val)[0]\n",
    "        if len(label_indices) > 0:\n",
    "            label_accuracy = accuracy_score(\n",
    "                y_true=y_true_mapped[label_indices], \n",
    "                y_pred=y_pred_mapped[label_indices]\n",
    "            )\n",
    "            print(f\"  - Accuracy for {name}: {label_accuracy:.3f}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot calculate metrics - no valid predictions made.\")\n",
    "    # Set default values if no predictions\n",
    "    accuracy = 0.0\n",
    "    precision_avg = 0.0\n",
    "    recall_avg = 0.0\n",
    "    f1_avg = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17244ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Confusion Matrix and Error Analysis\n",
    "# ============================================\n",
    "\n",
    "if len(y_pred) > 0:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🔍 CONFUSION MATRIX & ERROR ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "    print(\"\\n📊 Confusion Matrix:\")\n",
    "    print(\"Predicted →\")\n",
    "    print(\"Actual ↓     Non-sens  Secret\")\n",
    "    print(f\"Non-sens      {cm[0,0]:6d}   {cm[0,1]:6d}\")\n",
    "    print(f\"Secret        {cm[1,0]:6d}   {cm[1,1]:6d}\")\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Recall for Secret\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # Recall for Non-sensitive\n",
    "    \n",
    "    print(f\"\\n📊 Additional Metrics:\")\n",
    "    print(f\"  - True Positives (Secret correctly identified): {tp:,}\")\n",
    "    print(f\"  - True Negatives (Non-sensitive correctly identified): {tn:,}\")\n",
    "    print(f\"  - False Positives (Non-sensitive labeled as Secret): {fp:,}\")\n",
    "    print(f\"  - False Negatives (Secret labeled as Non-sensitive): {fn:,}\")\n",
    "    print(f\"  - Sensitivity (Secret recall): {sensitivity:.3f}\")\n",
    "    print(f\"  - Specificity (Non-sensitive recall): {specificity:.3f}\")\n",
    "    \n",
    "    # Error Analysis\n",
    "    print(f\"\\n🔍 Error Breakdown:\")\n",
    "    false_positives = [(true, pred, idx) for idx, (true, pred) in enumerate(zip(y_true_test, y_pred)) \n",
    "                      if true == 'Non-sensitive' and pred == 'Secret']\n",
    "    false_negatives = [(true, pred, idx) for idx, (true, pred) in enumerate(zip(y_true_test, y_pred)) \n",
    "                      if true == 'Secret' and pred == 'Non-sensitive']\n",
    "    \n",
    "    print(f\"  - False Positives: {len(false_positives):,} (Non-sensitive → Secret)\")\n",
    "    print(f\"  - False Negatives: {len(false_negatives):,} (Secret → Non-sensitive)\")\n",
    "    \n",
    "    # Show sample errors\n",
    "    if false_negatives:\n",
    "        print(f\"\\n❌ Sample False Negatives (Security Risk):\")\n",
    "        for i, (true, pred, idx) in enumerate(false_negatives[:3]):\n",
    "            candidate = X_test.iloc[idx]['candidate_string']\n",
    "            print(f\"  {i+1}. Candidate: {candidate[:80]}{'...' if len(candidate) > 80 else ''}\")\n",
    "            print(f\"     True: {true} → Predicted: {pred}\")\n",
    "    \n",
    "    if false_positives:\n",
    "        print(f\"\\n⚠️  Sample False Positives (Usability Impact):\")\n",
    "        for i, (true, pred, idx) in enumerate(false_positives[:3]):\n",
    "            candidate = X_test.iloc[idx]['candidate_string']\n",
    "            print(f\"  {i+1}. Candidate: {candidate[:80]}{'...' if len(candidate) > 80 else ''}\")\n",
    "            print(f\"     True: {true} → Predicted: {pred}\")\n",
    "    \n",
    "    # Risk Assessment\n",
    "    print(f\"\\n🚨 Risk Assessment:\")\n",
    "    if fn > 0:\n",
    "        fn_rate = fn / (tp + fn)\n",
    "        if fn_rate < 0.05:\n",
    "            print(f\"  ✅ LOW SECURITY RISK: False negative rate = {fn_rate:.3f}\")\n",
    "        elif fn_rate < 0.10:\n",
    "            print(f\"  ⚠️  MODERATE SECURITY RISK: False negative rate = {fn_rate:.3f}\")\n",
    "        else:\n",
    "            print(f\"  ❌ HIGH SECURITY RISK: False negative rate = {fn_rate:.3f}\")\n",
    "    \n",
    "    if fp > 0:\n",
    "        fp_rate = fp / (tn + fp)\n",
    "        if fp_rate < 0.05:\n",
    "            print(f\"  ✅ LOW USABILITY IMPACT: False positive rate = {fp_rate:.3f}\")\n",
    "        elif fp_rate < 0.10:\n",
    "            print(f\"  ⚠️  MODERATE USABILITY IMPACT: False positive rate = {fp_rate:.3f}\")\n",
    "        else:\n",
    "            print(f\"  ❌ HIGH USABILITY IMPACT: False positive rate = {fp_rate:.3f}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot perform error analysis - no valid predictions made.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b11a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Performance Summary and Results Saving\n",
    "# ============================================\n",
    "\n",
    "if len(y_pred) > 0:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"📋 PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Import required modules\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Create performance summary using correctly defined variables\n",
    "    performance_summary = {\n",
    "        'total_predictions': len(y_pred),\n",
    "        'successful_predictions': len([p for p in y_pred if p in ['Secret', 'Non-sensitive']]),\n",
    "        'failed_predictions': len([p for p in y_pred if p not in ['Secret', 'Non-sensitive']]),\n",
    "        'accuracy': accuracy,  # Now properly defined\n",
    "        'precision': precision_avg,  # Now properly defined\n",
    "        'recall': recall_avg,  # Now properly defined\n",
    "        'f1_score': f1_avg,  # Now properly defined\n",
    "        'false_negatives': fn if 'fn' in locals() else 0,\n",
    "        'false_positives': fp if 'fp' in locals() else 0,\n",
    "        'true_positives': tp if 'tp' in locals() else 0,\n",
    "        'true_negatives': tn if 'tn' in locals() else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Performance Overview:\")\n",
    "    print(f\"  - Total Test Samples: {performance_summary['total_predictions']:,}\")\n",
    "    print(f\"  - Successful Predictions: {performance_summary['successful_predictions']:,}\")\n",
    "    print(f\"  - Failed Predictions: {performance_summary['failed_predictions']:,}\")\n",
    "    print(f\"  - Accuracy: {performance_summary['accuracy']:.3f}\")\n",
    "    print(f\"  - Precision: {performance_summary['precision']:.3f}\")\n",
    "    print(f\"  - Recall: {performance_summary['recall']:.3f}\")\n",
    "    print(f\"  - F1-Score: {performance_summary['f1_score']:.3f}\")\n",
    "    \n",
    "    # Model quality assessment\n",
    "    print(f\"\\n🎯 Model Quality Assessment:\")\n",
    "    if accuracy >= 0.95:\n",
    "        print(\"  ✅ EXCELLENT performance (≥95% accuracy)\")\n",
    "    elif accuracy >= 0.90:\n",
    "        print(\"  ✅ GOOD performance (≥90% accuracy)\")\n",
    "    elif accuracy >= 0.80:\n",
    "        print(\"  ⚠️  ACCEPTABLE performance (≥80% accuracy)\")\n",
    "    else:\n",
    "        print(\"  ❌ POOR performance (<80% accuracy)\")\n",
    "    \n",
    "    # Save detailed results to CSV\n",
    "    try:\n",
    "        print(f\"\\n💾 Saving Results...\")\n",
    "        \n",
    "        # Prepare detailed results\n",
    "        detailed_results = []\n",
    "        for i, (true_label, pred_label) in enumerate(zip(y_true_test, y_pred)):\n",
    "            result = {\n",
    "                'index': i,\n",
    "                'candidate_string': X_test.iloc[i]['candidate_string'],\n",
    "                'true_label': true_label,\n",
    "                'predicted_label': pred_label,\n",
    "                'correct': true_label == pred_label,\n",
    "                'error_type': 'Correct' if true_label == pred_label else \n",
    "                             'False Positive' if true_label == 'Non-sensitive' and pred_label == 'Secret' else\n",
    "                             'False Negative' if true_label == 'Secret' and pred_label == 'Non-sensitive' else\n",
    "                             'Other Error'\n",
    "            }\n",
    "            detailed_results.append(result)\n",
    "        \n",
    "        # Save to CSV\n",
    "        results_df = pd.DataFrame(detailed_results)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        results_file = f\"training_test_results_{timestamp}.csv\"\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "        print(f\"  ✅ Detailed results saved to: {results_file}\")\n",
    "        \n",
    "        # Save performance summary\n",
    "        summary_file = f\"training_performance_summary_{timestamp}.json\"\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(performance_summary, f, indent=2)\n",
    "        print(f\"  ✅ Performance summary saved to: {summary_file}\")\n",
    "        \n",
    "        # Show error samples summary\n",
    "        error_samples = results_df[results_df['correct'] == False]\n",
    "        if len(error_samples) > 0:\n",
    "            print(f\"\\n🔍 Error Samples Preview:\")\n",
    "            print(f\"  - Total Errors: {len(error_samples):,}\")\n",
    "            print(f\"  - False Negatives: {len(error_samples[error_samples['error_type'] == 'False Negative']):,}\")\n",
    "            print(f\"  - False Positives: {len(error_samples[error_samples['error_type'] == 'False Positive']):,}\")\n",
    "            print(f\"  - Other Errors: {len(error_samples[error_samples['error_type'] == 'Other Error']):,}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving results: {e}\")\n",
    "    \n",
    "    print(f\"\\n🎉 TRAINING AND TESTING COMPLETE!\")\n",
    "    print(f\"Model saved at: {output_dir}\")\n",
    "    print(f\"Ready for deployment or further evaluation.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No valid predictions to summarize.\")\n",
    "    print(\"⚠️  Consider debugging the model inference process.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
