{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ae4dec",
   "metadata": {},
   "source": [
    "# Security String Classification Inference with DeepSeek 7B\n",
    "\n",
    "This notebook performs inference using a pre-trained security classification model. The model classifies strings as either \"Secret\" or \"Non-sensitive\" based on their context in issue reports.\n",
    "\n",
    "## Overview\n",
    "- **Task**: Binary classification of security-sensitive strings\n",
    "- **Model**: Fine-tuned DeepSeek 7B with LoRA adapters\n",
    "- **Framework**: Unsloth for efficient inference\n",
    "- **Data**: CSV format with candidate strings and issue reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3cdda7",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Imports\n",
    "\n",
    "Import all necessary libraries and set up the environment for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb3824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set environment for Unsloth\n",
    "os.environ[\"UNSLOTH_IS_PRESENT\"] = \"1\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(69420)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model results directory\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create timestamp for this run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = \"deepseek-7b-security-classifier\"\n",
    "results_dir = f\"model_results/{model_name}/{timestamp}\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(f\"{results_dir}/metrics\", exist_ok=True)\n",
    "os.makedirs(f\"{results_dir}/analysis\", exist_ok=True)\n",
    "os.makedirs(f\"{results_dir}/predictions\", exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Created model results directory: {results_dir}\")\n",
    "print(f\"üìÅ Directory structure:\")\n",
    "print(f\"  - {results_dir}/metrics/     (performance metrics)\")\n",
    "print(f\"  - {results_dir}/analysis/    (error analysis)\")\n",
    "print(f\"  - {results_dir}/predictions/ (detailed predictions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7676a0",
   "metadata": {},
   "source": [
    "## Step 2: Load Test Data\n",
    "\n",
    "Load the test dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d553f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "df_test = pd.read_csv(\"../Data/test.csv\")\n",
    "\n",
    "print(f\"Test shape: {df_test.shape}\")\n",
    "\n",
    "# Display data structure\n",
    "print(\"\\nüìä Test data sample:\")\n",
    "print(df_test.head(2))\n",
    "\n",
    "print(\"\\nüìã Column information:\")\n",
    "print(df_test.columns.tolist())\n",
    "print(f\"\\nData types:\\n{df_test.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe2ca6f",
   "metadata": {},
   "source": [
    "## Step 3: Data Preprocessing\n",
    "\n",
    "Apply the same preprocessing as used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_context_window(text, target_string, window_size=200):\n",
    "    \"\"\"Create context window around target string\"\"\"\n",
    "    target_index = text.find(target_string)\n",
    "    if target_index != -1:\n",
    "        start_index = max(0, target_index - window_size)\n",
    "        end_index = min(len(text), target_index + len(target_string) + window_size)\n",
    "        context_window = text[start_index:end_index]\n",
    "        return context_window\n",
    "    return None\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"üîÑ Preprocessing test data...\")\n",
    "df_test['modified_text'] = df_test.apply(lambda row: create_context_window(row['text'], row['candidate_string']), axis=1)\n",
    "\n",
    "# Convert labels to text format\n",
    "df_test['label'] = df_test['label'].replace({0: 'Non-sensitive', 1: 'Secret'})\n",
    "\n",
    "print(\"‚úÖ Data preprocessing complete!\")\n",
    "print(f\"üìä Label distribution in test data:\")\n",
    "print(df_test['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed62ef4",
   "metadata": {},
   "source": [
    "## Step 4: Load Pre-trained Model\n",
    "\n",
    "Load the saved fine-tuned model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ffae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "max_seq_length = 1024\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# Path to the saved model (adjust as needed)\n",
    "model_path = \"../models/deepseek-7b-ft-unsloth_merged\"  # Use the merged model for easier inference\n",
    "\n",
    "print(\"üîß Loading fine-tuned model...\")\n",
    "print(f\"Model path: {model_path}\")\n",
    "print(f\"Max sequence length: {max_seq_length}\")\n",
    "print(f\"4-bit quantization: {load_in_4bit}\")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_path,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "# Set model to inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully and set to inference mode!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7de395",
   "metadata": {},
   "source": [
    "## Step 5: Inference Functions\n",
    "\n",
    "Define the inference functions using the same prompt template as training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a6cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt_for_inference(candidate_string, issue_report):\n",
    "    \"\"\"Format prompt for inference using DeepSeek's chat format\"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a security auditor or classifier specialized in identifying and categorizing sensitive secrets from issue reports.. Classify the given candidate string as either \"Non-sensitive\" or \"Secret\" based on its context.\n",
    "\n",
    "A \"Secret\" includes sensitive information such as: \n",
    "- API keys and secrets (e.g., `sk_test_ABC123`)  \n",
    "- Private and secret keys (e.g., private SSH keys, private cryptographic keys)  \n",
    "- Authentication keys and tokens (e.g., `Bearer <token>`)  \n",
    "- Database connection strings with credentials (e.g., `mongodb://user:password@host:port`)  \n",
    "- Passwords, usernames, and any other private information that should not be shared openly.  \n",
    "\n",
    "A \"Non-sensitive\" string is not considered secret and can be shared openly. This includes:  \n",
    "- Public keys of any form (e.g., public SSH keys)  \n",
    "- Non-sensitive configuration values or identifiers  \n",
    "- Actual-looking keys that are clearly marked as dummy/test (e.g., with comments like '# dummy key' or variable names like 'test_key')  \n",
    "- Strings that just look random or patterned but are not actually secrets (e.g., `xyz123`, 'xxxx', `abc123`, `EXAMPLE_KEY`, `token_value`)  \n",
    "- Strings that are clearly placeholders or redacted text (e.g., 'XXXXXXXX', '[REDACTED]', '[TRUNCATED]')  \n",
    "- **Obfuscated or masked values (e.g., '****', '****123', 'abc...xyz')**  \n",
    "\n",
    "These are always considered **\"Non-sensitive\"**, even if they appear in a sensitive context.\n",
    "\n",
    "Reply with only the classification: \"Non-sensitive\" or \"Secret\".\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Classify the given candidate string based on its role in the provided issue report.\n",
    "\n",
    "candidate_string: {candidate_string}\n",
    "issue_report: {issue_report}\"\"\"\n",
    "\n",
    "    # Inference format using DeepSeek's chat style\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "{system_prompt}<|im_end|>\n",
    "<|im_start|>user\n",
    "{user_prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def extract_label(model_response):\n",
    "    \"\"\"Extract label from model response\"\"\"\n",
    "    if \"Secret\" in model_response:\n",
    "        return \"Secret\"\n",
    "    else:\n",
    "        return \"Non-sensitive\"\n",
    "\n",
    "def predict_single(candidate_string, issue_report, model, tokenizer):\n",
    "    \"\"\"Single prediction function for testing\"\"\"\n",
    "    # Format prompt for inference\n",
    "    test_prompt = format_prompt_for_inference(candidate_string, issue_report)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        test_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length\n",
    "    )\n",
    "    \n",
    "    # Move to GPU\n",
    "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_new_tokens=5,\n",
    "            temperature=0.1,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=False\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    assistant_marker = '<|im_start|>assistant'\n",
    "    if assistant_marker in response:\n",
    "        model_response = response.split(assistant_marker)[-1].strip()\n",
    "    else:\n",
    "        model_response = response[len(test_prompt):].strip()\n",
    "    \n",
    "    predicted_label = extract_label(model_response)\n",
    "    return predicted_label, model_response\n",
    "\n",
    "print(\"‚úÖ Inference functions defined!\")\n",
    "print(\"  - format_prompt_for_inference: Formats prompts for inference\")\n",
    "print(\"  - extract_label: Extracts classification labels from model responses\")\n",
    "print(\"  - predict_single: Makes single predictions with the trained model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ccf74",
   "metadata": {},
   "source": [
    "## Step 6: Batch Prediction Function\n",
    "\n",
    "Define batch prediction function for efficient processing of multiple samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(test_df, model, tokenizer, batch_size=8):\n",
    "    \"\"\"Batch prediction function for comprehensive testing\"\"\"\n",
    "    y_pred = []\n",
    "    errors = []\n",
    "    \n",
    "    print(f\"üîÑ Running batch predictions on {len(test_df):,} examples...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(test_df), batch_size), desc=\"Predicting\"):\n",
    "        batch = test_df.iloc[i:i+batch_size]\n",
    "        \n",
    "        for idx, row in batch.iterrows():\n",
    "            try:\n",
    "                predicted_label, _ = predict_single(\n",
    "                    row[\"candidate_string\"], \n",
    "                    row[\"modified_text\"], \n",
    "                    model, \n",
    "                    tokenizer\n",
    "                )\n",
    "                y_pred.append(predicted_label)\n",
    "            except Exception as e:\n",
    "                errors.append(f\"Error at index {idx}: {e}\")\n",
    "                y_pred.append(\"Non-sensitive\")  # Default prediction\n",
    "                continue\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"‚ö†Ô∏è  {len(errors)} errors occurred during prediction:\")\n",
    "        for error in errors[:3]:  # Show first 3 errors\n",
    "            print(f\"  - {error}\")\n",
    "        if len(errors) > 3:\n",
    "            print(f\"  - ... and {len(errors) - 3} more errors\")\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "print(\"‚úÖ Batch prediction function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3aa201",
   "metadata": {},
   "source": [
    "## Step 7: Run Comprehensive Evaluation\n",
    "\n",
    "Execute the model on the test set and collect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0cbcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive evaluation on test set\n",
    "print(\"üöÄ Running comprehensive evaluation on test set...\")\n",
    "\n",
    "# Get predictions for the entire test set\n",
    "X_test = df_test\n",
    "y_pred = predict_batch(X_test, model, tokenizer)\n",
    "y_true_test = X_test['label'].tolist()\n",
    "\n",
    "print(f\"‚úÖ Evaluation completed!\")\n",
    "print(f\"üìä Prediction Summary:\")\n",
    "print(f\"  - Total predictions: {len(y_pred):,}\")\n",
    "print(f\"  - Unique predicted labels: {set(y_pred)}\")\n",
    "print(f\"  - True label distribution: {X_test['label'].value_counts().to_dict()}\")\n",
    "\n",
    "# Quick accuracy check\n",
    "correct_predictions = sum(1 for true, pred in zip(y_true_test, y_pred) if true == pred)\n",
    "quick_accuracy = correct_predictions / len(y_pred) if len(y_pred) > 0 else 0\n",
    "print(f\"  - Quick accuracy: {quick_accuracy:.3f} ({correct_predictions}/{len(y_pred)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc48490",
   "metadata": {},
   "source": [
    "## Step 8: Detailed Performance Metrics\n",
    "\n",
    "Calculate and display comprehensive performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb95a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Detailed Performance Metrics\n",
    "# ============================================\n",
    "accuracy = 0.0\n",
    "precision_avg = 0.0\n",
    "recall_avg = 0.0\n",
    "f1_avg = 0.0\n",
    "\n",
    "if len(y_pred) > 0:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìà DETAILED PERFORMANCE METRICS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Classification Report with 3 decimal places\n",
    "    print(\"\\nüìä Classification Report:\")\n",
    "    print(classification_report(y_true_test, y_pred, digits=3))\n",
    "    \n",
    "    # Calculate precision, recall, F1-score for each class\n",
    "    labels = sorted(set(y_true_test))\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true_test, \n",
    "        y_pred, \n",
    "        labels=labels\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüè∑Ô∏è  Per-Class Detailed Metrics:\")\n",
    "    for i, label in enumerate(labels):\n",
    "        print(f\"\\n  {label.upper()}:\")\n",
    "        print(f\"    - Precision: {precision[i]:.3f}\")\n",
    "        print(f\"    - Recall: {recall[i]:.3f}\")\n",
    "        print(f\"    - F1-score: {f1[i]:.3f}\")\n",
    "        print(f\"    - Support: {support[i]:,}\")\n",
    "    \n",
    "    # Overall accuracy and binary metrics\n",
    "    def map_func(x):\n",
    "        return 1 if x == \"Secret\" else 0\n",
    "\n",
    "    y_true_mapped = np.array([map_func(label) for label in y_true_test])\n",
    "    y_pred_mapped = np.array([map_func(label) for label in y_pred])\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    overall_accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "    \n",
    "    # Calculate weighted averages for overall metrics\n",
    "    precision_overall, recall_overall, f1_overall, _ = precision_recall_fscore_support(\n",
    "        y_true_mapped, y_pred_mapped, average='weighted'\n",
    "    )\n",
    "    \n",
    "    # Store these for later use in performance summary\n",
    "    accuracy = overall_accuracy\n",
    "    precision_avg = precision_overall\n",
    "    recall_avg = recall_overall\n",
    "    f1_avg = f1_overall\n",
    "    \n",
    "    print(f\"\\nüéØ Overall Performance:\")\n",
    "    print(f\"  - Overall Accuracy: {overall_accuracy:.3f}\")\n",
    "    print(f\"  - Weighted Precision: {precision_overall:.3f}\")\n",
    "    print(f\"  - Weighted Recall: {recall_overall:.3f}\")\n",
    "    print(f\"  - Weighted F1-Score: {f1_overall:.3f}\")\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    for label_val, name in zip([0, 1], [\"Non-sensitive\", \"Secret\"]):\n",
    "        label_indices = np.where(y_true_mapped == label_val)[0]\n",
    "        if len(label_indices) > 0:\n",
    "            label_accuracy = accuracy_score(\n",
    "                y_true=y_true_mapped[label_indices], \n",
    "                y_pred=y_pred_mapped[label_indices]\n",
    "            )\n",
    "            print(f\"  - Accuracy for {name}: {label_accuracy:.3f}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot calculate metrics - no valid predictions made.\")\n",
    "    # Set default values if no predictions\n",
    "    accuracy = 0.0\n",
    "    precision_avg = 0.0\n",
    "    recall_avg = 0.0\n",
    "    f1_avg = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed02aecc",
   "metadata": {},
   "source": [
    "## Step 9: Confusion Matrix and Error Analysis\n",
    "\n",
    "Analyze prediction errors and create confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Confusion Matrix and Error Analysis\n",
    "# ============================================\n",
    "\n",
    "if len(y_pred) > 0:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üîç CONFUSION MATRIX & ERROR ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "    print(\"\\nüìä Confusion Matrix:\")\n",
    "    print(\"Predicted ‚Üí\")\n",
    "    print(\"Actual ‚Üì     Non-sens  Secret\")\n",
    "    print(f\"Non-sens      {cm[0,0]:6d}   {cm[0,1]:6d}\")\n",
    "    print(f\"Secret        {cm[1,0]:6d}   {cm[1,1]:6d}\")\n",
    "    \n",
    "    # Calculate derived metrics\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0  # Recall for Secret\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # Recall for Non-sensitive\n",
    "    \n",
    "    print(f\"\\nüìä Additional Metrics:\")\n",
    "    print(f\"  - True Positives (Secret correctly identified): {tp:,}\")\n",
    "    print(f\"  - True Negatives (Non-sensitive correctly identified): {tn:,}\")\n",
    "    print(f\"  - False Positives (Non-sensitive labeled as Secret): {fp:,}\")\n",
    "    print(f\"  - False Negatives (Secret labeled as Non-sensitive): {fn:,}\")\n",
    "    print(f\"  - Sensitivity (Secret recall): {sensitivity:.3f}\")\n",
    "    print(f\"  - Specificity (Non-sensitive recall): {specificity:.3f}\")\n",
    "    \n",
    "    # Error Analysis\n",
    "    print(f\"\\nüîç Error Breakdown:\")\n",
    "    false_positives = [(true, pred, idx) for idx, (true, pred) in enumerate(zip(y_true_test, y_pred)) \n",
    "                      if true == 'Non-sensitive' and pred == 'Secret']\n",
    "    false_negatives = [(true, pred, idx) for idx, (true, pred) in enumerate(zip(y_true_test, y_pred)) \n",
    "                      if true == 'Secret' and pred == 'Non-sensitive']\n",
    "    \n",
    "    print(f\"  - False Positives: {len(false_positives):,} (Non-sensitive ‚Üí Secret)\")\n",
    "    print(f\"  - False Negatives: {len(false_negatives):,} (Secret ‚Üí Non-sensitive)\")\n",
    "    \n",
    "    # Show sample errors\n",
    "    if false_negatives:\n",
    "        print(f\"\\n‚ùå Sample False Negatives (Security Risk):\")\n",
    "        for i, (true, pred, idx) in enumerate(false_negatives[:3]):\n",
    "            candidate = X_test.iloc[idx]['candidate_string']\n",
    "            print(f\"  {i+1}. Candidate: {candidate[:80]}{'...' if len(candidate) > 80 else ''}\")\n",
    "            print(f\"     True: {true} ‚Üí Predicted: {pred}\")\n",
    "    \n",
    "    if false_positives:\n",
    "        print(f\"\\n‚ö†Ô∏è  Sample False Positives (Usability Impact):\")\n",
    "        for i, (true, pred, idx) in enumerate(false_positives[:3]):\n",
    "            candidate = X_test.iloc[idx]['candidate_string']\n",
    "            print(f\"  {i+1}. Candidate: {candidate[:80]}{'...' if len(candidate) > 80 else ''}\")\n",
    "            print(f\"     True: {true} ‚Üí Predicted: {pred}\")\n",
    "    \n",
    "    # Risk Assessment\n",
    "    print(f\"\\nüö® Risk Assessment:\")\n",
    "    if fn > 0:\n",
    "        fn_rate = fn / (tp + fn)\n",
    "        if fn_rate < 0.05:\n",
    "            print(f\"  ‚úÖ LOW SECURITY RISK: False negative rate = {fn_rate:.3f}\")\n",
    "        elif fn_rate < 0.10:\n",
    "            print(f\"  ‚ö†Ô∏è  MODERATE SECURITY RISK: False negative rate = {fn_rate:.3f}\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå HIGH SECURITY RISK: False negative rate = {fn_rate:.3f}\")\n",
    "    \n",
    "    if fp > 0:\n",
    "        fp_rate = fp / (tn + fp)\n",
    "        if fp_rate < 0.05:\n",
    "            print(f\"  ‚úÖ LOW USABILITY IMPACT: False positive rate = {fp_rate:.3f}\")\n",
    "        elif fp_rate < 0.10:\n",
    "            print(f\"  ‚ö†Ô∏è  MODERATE USABILITY IMPACT: False positive rate = {fp_rate:.3f}\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå HIGH USABILITY IMPACT: False positive rate = {fp_rate:.3f}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot perform error analysis - no valid predictions made.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbe8964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Detailed False Positives and False Negatives Analysis\n",
    "# ============================================\n",
    "\n",
    "if len(y_pred) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîç DETAILED FALSE POSITIVES AND FALSE NEGATIVES ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Identify all error cases\n",
    "    false_positives = []\n",
    "    false_negatives = []\n",
    "    \n",
    "    for idx, (true_label, pred_label) in enumerate(zip(y_true_test, y_pred)):\n",
    "        if true_label == 'Non-sensitive' and pred_label == 'Secret':\n",
    "            false_positives.append({\n",
    "                'index': idx,\n",
    "                'candidate_string': X_test.iloc[idx]['candidate_string'],\n",
    "                'context_window': X_test.iloc[idx]['modified_text'],\n",
    "                'actual_label': true_label,\n",
    "                'predicted_label': pred_label\n",
    "            })\n",
    "        elif true_label == 'Secret' and pred_label == 'Non-sensitive':\n",
    "            false_negatives.append({\n",
    "                'index': idx,\n",
    "                'candidate_string': X_test.iloc[idx]['candidate_string'],\n",
    "                'context_window': X_test.iloc[idx]['modified_text'],\n",
    "                'actual_label': true_label,\n",
    "                'predicted_label': pred_label\n",
    "            })\n",
    "    \n",
    "    # FALSE NEGATIVES Analysis (High Security Risk)\n",
    "    print(f\"\\n‚ùå FALSE NEGATIVES (Secrets missed - HIGH SECURITY RISK)\")\n",
    "    print(f\"Total Count: {len(false_negatives)}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if false_negatives:\n",
    "        for i, fn_case in enumerate(false_negatives[:10]):  # Show top 10\n",
    "            print(f\"\\n{i+1}. Index: {fn_case['index']}\")\n",
    "            candidate_string = X_test.iloc[fn_case['index']]['candidate_string']\n",
    "            print(f\"   Candidate: '{candidate_string}'\")\n",
    "            print(f\"   Actual: {fn_case['actual_label']} ‚Üí Predicted: {fn_case['predicted_label']}\")\n",
    "            context_preview = fn_case['context_window'][:200] + '...' if len(fn_case['context_window']) > 200 else fn_case['context_window']\n",
    "            print(f\"   Context: {context_preview}\")\n",
    "            print(f\"   Risk: ‚ö†Ô∏è  SECURITY BREACH - Secret not detected!\")\n",
    "        \n",
    "        if len(false_negatives) > 10:\n",
    "            print(f\"\\n... and {len(false_negatives) - 10} more false negatives\")\n",
    "        \n",
    "        # Save false negatives to CSV with context window, actual label, and predicted label\n",
    "        if 'results_dir' in locals():\n",
    "            fn_df = pd.DataFrame(false_negatives)\n",
    "            fn_file = f\"{results_dir}/analysis/false_negatives.csv\"\n",
    "            fn_df.to_csv(fn_file, index=False)\n",
    "            print(f\"\\nüìÑ False negatives saved to: {fn_file}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No false negatives found! Perfect security detection.\")\n",
    "    \n",
    "    # FALSE POSITIVES Analysis (Usability Impact)\n",
    "    print(f\"\\n‚ö†Ô∏è  FALSE POSITIVES (Non-secrets flagged - USABILITY IMPACT)\")\n",
    "    print(f\"Total Count: {len(false_positives)}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if false_positives:\n",
    "        for i, fp_case in enumerate(false_positives[:10]):  # Show top 10\n",
    "            print(f\"\\n{i+1}. Index: {fp_case['index']}\")\n",
    "            candidate_string = X_test.iloc[fp_case['index']]['candidate_string']\n",
    "            print(f\"   Candidate: '{candidate_string}'\")\n",
    "            print(f\"   Actual: {fp_case['actual_label']} ‚Üí Predicted: {fp_case['predicted_label']}\")\n",
    "            context_preview = fp_case['context_window'][:200] + '...' if len(fp_case['context_window']) > 200 else fp_case['context_window']\n",
    "            print(f\"   Context: {context_preview}\")\n",
    "            print(f\"   Impact: üì¢ USABILITY - Non-secret flagged as secret\")\n",
    "        \n",
    "        if len(false_positives) > 10:\n",
    "            print(f\"\\n... and {len(false_positives) - 10} more false positives\")\n",
    "        \n",
    "        # Save false positives to CSV with context window, actual label, and predicted label\n",
    "        if 'results_dir' in locals():\n",
    "            fp_df = pd.DataFrame(false_positives)\n",
    "            fp_file = f\"{results_dir}/analysis/false_positives.csv\"\n",
    "            fp_df.to_csv(fp_file, index=False)\n",
    "            print(f\"\\nüìÑ False positives saved to: {fp_file}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No false positives found! Perfect precision.\")\n",
    "    \n",
    "    # Pattern Analysis\n",
    "    print(f\"\\nüîç PATTERN ANALYSIS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if false_negatives:\n",
    "        fn_candidate_strings = [X_test.iloc[case['index']]['candidate_string'] for case in false_negatives]\n",
    "        fn_lengths = [len(candidate) for candidate in fn_candidate_strings]\n",
    "        print(f\"False Negatives - Candidate String Lengths:\")\n",
    "        print(f\"  Average: {np.mean(fn_lengths):.1f} chars\")\n",
    "        print(f\"  Range: {min(fn_lengths)} - {max(fn_lengths)} chars\")\n",
    "    \n",
    "    if false_positives:\n",
    "        fp_candidate_strings = [X_test.iloc[case['index']]['candidate_string'] for case in false_positives]\n",
    "        fp_lengths = [len(candidate) for candidate in fp_candidate_strings]\n",
    "        print(f\"False Positives - Candidate String Lengths:\")\n",
    "        print(f\"  Average: {np.mean(fp_lengths):.1f} chars\")\n",
    "        print(f\"  Range: {min(fp_lengths)} - {max(fp_lengths)} chars\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Cannot perform false positives/negatives analysis - no valid predictions made.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a3eed7",
   "metadata": {},
   "source": [
    "## Step 10: Performance Summary and Results Saving\n",
    "\n",
    "Generate final performance summary and save results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982721fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Performance Summary and Results Saving\n",
    "# ============================================\n",
    "\n",
    "if len(y_pred) > 0:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìã PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create performance summary using correctly defined variables\n",
    "    performance_summary = {\n",
    "        'total_predictions': len(y_pred),\n",
    "        'successful_predictions': len([p for p in y_pred if p in ['Secret', 'Non-sensitive']]),\n",
    "        'failed_predictions': len([p for p in y_pred if p not in ['Secret', 'Non-sensitive']]),\n",
    "        'accuracy': float(accuracy),\n",
    "        'precision': float(precision_avg),\n",
    "        'recall': float(recall_avg),\n",
    "        'f1_score': float(f1_avg),\n",
    "        'false_negatives': int(fn) if 'fn' in locals() else 0,\n",
    "        'false_positives': int(fp) if 'fp' in locals() else 0,\n",
    "        'true_positives': int(tp) if 'tp' in locals() else 0,\n",
    "        'true_negatives': int(tn) if 'tn' in locals() else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Performance Overview:\")\n",
    "    print(f\"  - Total Test Samples: {performance_summary['total_predictions']:,}\")\n",
    "    print(f\"  - Successful Predictions: {performance_summary['successful_predictions']:,}\")\n",
    "    print(f\"  - Failed Predictions: {performance_summary['failed_predictions']:,}\")\n",
    "    print(f\"  - Accuracy: {performance_summary['accuracy']:.3f}\")\n",
    "    print(f\"  - Precision: {performance_summary['precision']:.3f}\")\n",
    "    print(f\"  - Recall: {performance_summary['recall']:.3f}\")\n",
    "    print(f\"  - F1-Score: {performance_summary['f1_score']:.3f}\")\n",
    "    \n",
    "    # Model quality assessment\n",
    "    print(f\"\\nüéØ Model Quality Assessment:\")\n",
    "    if accuracy >= 0.95:\n",
    "        print(\"  ‚úÖ EXCELLENT performance (‚â•95% accuracy)\")\n",
    "    elif accuracy >= 0.90:\n",
    "        print(\"  ‚úÖ GOOD performance (‚â•90% accuracy)\")\n",
    "    elif accuracy >= 0.80:\n",
    "        print(\"  ‚ö†Ô∏è  ACCEPTABLE performance (‚â•80% accuracy)\")\n",
    "    else:\n",
    "        print(\"  ‚ùå POOR performance (<80% accuracy)\")\n",
    "    \n",
    "    # Save detailed results to CSV\n",
    "    try:\n",
    "        print(f\"\\nüíæ Saving Results to {results_dir}...\")\n",
    "        \n",
    "        # Prepare detailed results with enhanced information\n",
    "        detailed_results = []\n",
    "        for i, (true_label, pred_label) in enumerate(zip(y_true_test, y_pred)):\n",
    "            result = {\n",
    "                'index': i,\n",
    "                'candidate_string': X_test.iloc[i]['candidate_string'],\n",
    "                'context_window': X_test.iloc[i]['modified_text'],\n",
    "                'original_text': X_test.iloc[i]['text'] if 'text' in X_test.columns else '',\n",
    "                'actual_label': true_label,\n",
    "                'predicted_label': pred_label,\n",
    "                'correct': true_label == pred_label,\n",
    "                'error_type': 'Correct' if true_label == pred_label else \n",
    "                             'False Positive' if true_label == 'Non-sensitive' and pred_label == 'Secret' else\n",
    "                             'False Negative' if true_label == 'Secret' and pred_label == 'Non-sensitive' else\n",
    "                             'Other Error',\n",
    "                'candidate_length': len(X_test.iloc[i]['candidate_string']),\n",
    "                'context_length': len(X_test.iloc[i]['modified_text']) if X_test.iloc[i]['modified_text'] else 0\n",
    "            }\n",
    "            detailed_results.append(result)\n",
    "        \n",
    "        # Save detailed predictions to CSV\n",
    "        results_df = pd.DataFrame(detailed_results)\n",
    "        results_file = f\"{results_dir}/predictions/detailed_predictions.csv\"\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "        print(f\"  ‚úÖ Detailed results saved to: {results_file}\")\n",
    "        \n",
    "        # Save performance summary\n",
    "        summary_file = f\"{results_dir}/metrics/performance_summary.json\"\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(performance_summary, f, indent=2)\n",
    "        print(f\"  ‚úÖ Performance summary saved to: {summary_file}\")\n",
    "        \n",
    "        # Show error samples summary\n",
    "        error_samples = results_df[results_df['correct'] == False]\n",
    "        if len(error_samples) > 0:\n",
    "            print(f\"\\nüîç Error Samples Preview:\")\n",
    "            print(f\"  - Total Errors: {len(error_samples):,}\")\n",
    "            print(f\"  - False Negatives: {len(error_samples[error_samples['error_type'] == 'False Negative']):,}\")\n",
    "            print(f\"  - False Positives: {len(error_samples[error_samples['error_type'] == 'False Positive']):,}\")\n",
    "            print(f\"  - Other Errors: {len(error_samples[error_samples['error_type'] == 'Other Error']):,}\")\n",
    "            \n",
    "            # Save error samples separately\n",
    "            error_file = f\"{results_dir}/analysis/error_samples.csv\"\n",
    "            error_samples.to_csv(error_file, index=False)\n",
    "            print(f\"  ‚úÖ Error samples saved to: {error_file}\")\n",
    "        \n",
    "        # Save classification report as text file with 3 decimal places\n",
    "        report_file = f\"{results_dir}/metrics/classification_report.txt\"\n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(\"SECURITY STRING CLASSIFICATION - DETAILED PERFORMANCE REPORT\\n\")\n",
    "            f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "            f.write(f\"Model: {model_name}\\n\")\n",
    "            f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "            f.write(f\"Test Samples: {len(y_pred):,}\\n\\n\")\n",
    "            f.write(\"CLASSIFICATION REPORT:\\n\")\n",
    "            f.write(classification_report(y_true_test, y_pred, digits=3))\n",
    "            f.write(f\"\\n\\nOVERALL METRICS:\\n\")\n",
    "            f.write(f\"Accuracy: {accuracy:.3f}\\n\")\n",
    "            f.write(f\"Precision: {precision_avg:.3f}\\n\")\n",
    "            f.write(f\"Recall: {recall_avg:.3f}\\n\")\n",
    "            f.write(f\"F1-Score: {f1_avg:.3f}\\n\")\n",
    "        print(f\"  ‚úÖ Classification report saved to: {report_file}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving results: {e}\")\n",
    "    \n",
    "    print(f\"\\nüéâ INFERENCE TESTING COMPLETE!\")\n",
    "    print(f\"Model evaluated successfully on {len(y_pred):,} test samples.\")\n",
    "    print(f\"\\nüìÅ All results saved to: {results_dir}\")\n",
    "    print(f\"üìä Files created:\")\n",
    "    print(f\"  - predictions/detailed_predictions.csv (enhanced with context and metadata)\")\n",
    "    print(f\"  - metrics/performance_summary.json\")\n",
    "    print(f\"  - metrics/classification_report.txt\") \n",
    "    print(f\"  - analysis/error_samples.csv\")\n",
    "    print(f\"  - analysis/false_negatives.csv (context_window, actual_label, predicted_label)\")\n",
    "    print(f\"  - analysis/false_positives.csv (context_window, actual_label, predicted_label)\")\n",
    "    print(f\"\\n‚úÖ Ready for deployment or further analysis.\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No valid predictions to summarize.\")\n",
    "    print(\"‚ö†Ô∏è  Consider debugging the model inference process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ce6caf",
   "metadata": {},
   "source": [
    "## Step 11: Single Sample Testing (Optional)\n",
    "\n",
    "Test the model on individual samples for interactive exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee50bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test individual sample\n",
    "sample_idx = 0  # Change this to test different samples\n",
    "sample_row = X_test.iloc[sample_idx]\n",
    "\n",
    "print(\"üîç Single Sample Testing\")\n",
    "print(f\"Sample Index: {sample_idx}\")\n",
    "print(f\"Candidate String: {sample_row['candidate_string']}\")\n",
    "print(f\"True Label: {sample_row['label']}\")\n",
    "print(f\"Context Preview: {sample_row['modified_text'][:200]}...\")\n",
    "\n",
    "# Make prediction\n",
    "predicted_label, full_response = predict_single(\n",
    "    sample_row['candidate_string'],\n",
    "    sample_row['modified_text'],\n",
    "    model,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "print(f\"\\nüìù Model Response: {full_response[:100]}...\")\n",
    "print(f\"üéØ Predicted Label: {predicted_label}\")\n",
    "print(f\"‚úÖ Correct: {predicted_label == sample_row['label']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
