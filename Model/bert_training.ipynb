{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "RANDOM_SEED = 42 # Your chosen seed\n",
    "\n",
    "def set_all_seeds(seed_value):\n",
    "    print(f\"Setting all seeds to: {seed_value}\")\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value) # Set it for consistent hashing\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value) # for multi-GPU.\n",
    "        torch.cuda.manual_seed(seed_value)     # for current GPU.\n",
    "\n",
    "# Call this AT THE VERY BEGINNING of your script, before almost any other import or operation\n",
    "set_all_seeds(RANDOM_SEED)\n",
    "\n",
    "# ... then your other imports like pandas, tqdm, sklearn, transformers ...\n",
    "# ... then your model definition, tokenizer, dataset, dataloader creation ...\n",
    "# ... then your training loop ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1707329129042,
     "user": {
      "displayName": "Sadif Ahmed",
      "userId": "17613161859460678714"
     },
     "user_tz": -360
    },
    "id": "SPs0eg7jqSVN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context Window Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1707329188201,
     "user": {
      "displayName": "Sadif Ahmed",
      "userId": "17613161859460678714"
     },
     "user_tz": -360
    },
    "id": "JjYKTmpsEbWp",
    "outputId": "763d1d86-a823-4f56-d720-fc85881c5ea7"
   },
   "outputs": [],
   "source": [
    "def create_context_window(text, target_string, window_size=200):\n",
    "\n",
    "    target_index = text.find(target_string)\n",
    "\n",
    "    if target_index != -1:\n",
    "        start_index = max(0, target_index - window_size)\n",
    "        end_index = min(len(text), target_index + len(target_string) + window_size)\n",
    "        context_window = text[start_index:end_index]\n",
    "        return context_window\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def clean_text(text, remove_non_printable=True):\n",
    "    \"\"\"\n",
    "    Clean a single text string for BART fine-tuning.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): The input text.\n",
    "    - remove_non_printable (bool): Whether to remove non-printable characters.\n",
    "    \n",
    "    Returns:\n",
    "    - str: Cleaned text.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Strip leading/trailing whitespace\n",
    "    cleaned = text.strip()\n",
    "    \n",
    "    # 2. Replace multiple newlines/tabs with a space\n",
    "    cleaned = re.sub(r'[\\r\\n\\t]+', ' ', cleaned)\n",
    "    \n",
    "    # 3. Remove excessive spaces\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    \n",
    "    # 4. Optionally remove embedded <eos> or </s> tokens (Bart uses </s> as EOS)\n",
    "    cleaned = re.sub(r'(</s>|<eos>)', '', cleaned)\n",
    "    \n",
    "    # 5. Optionally remove non-printable characters\n",
    "    if remove_non_printable:\n",
    "        printable_chars = set(string.printable)\n",
    "        cleaned = ''.join(filter(lambda x: x in printable_chars, cleaned))\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_dataframe(input_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Prepare X_text, X_candidate, Y_labels from a raw dataframe.\n",
    "    - Cleans NaN/Â±inf\n",
    "    - Casts text/candidate to string\n",
    "    - Casts label to int (dropping NaN labels)\n",
    "    - Builds clean_text and modified_text columns\n",
    "    Returns: (X_text, X_candidate, Y_labels)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) Validate input ---\n",
    "    if not isinstance(input_df, pd.DataFrame):\n",
    "        raise TypeError(\"Input must be a Pandas DataFrame.\")\n",
    "    required_cols = {\"Issue_id\",\"text\", \"candidate_string\", \"label\"}\n",
    "    missing = required_cols - set(input_df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required column(s): {missing}\")\n",
    "\n",
    "    if input_df.empty:\n",
    "        print(\"Warning: Input DataFrame is empty.\")\n",
    "        return [], [], []\n",
    "\n",
    "    # --- 2) Work on a copy ---\n",
    "    df = input_df.copy()\n",
    "\n",
    "    # --- 3) Clean candidate_string & text: replace inf, drop NaN if you need strict non-empty ---\n",
    "    for col in [\"candidate_string\", \"text\"]:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "        # If you prefer dropping rows where these are missing, uncomment next line:\n",
    "        df = df.dropna(subset=[col])\n",
    "        # Otherwise, keep rows but fill with empty string to avoid tokenizer errors:\n",
    "        #df[col] = df[col].astype(\"string\").fillna(\"\")\n",
    "\n",
    "    # --- 4) Clean label: replace infâ†’NaN, drop NaN, cast to int ---\n",
    "    df[\"label\"] = df[\"label\"].replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.dropna(subset=[\"label\"])\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "    # --- 5) Build clean_text then modified_text ---\n",
    "    # Ensure clean_text always returns str; wrap if needed\n",
    "    def _clean_safe(x):\n",
    "        try:\n",
    "            return clean_text(\"\" if x is None else str(x))\n",
    "        except Exception:\n",
    "            # Fallback: still return string to avoid downstream type errors\n",
    "            return \"\" if x is None else str(x)\n",
    "\n",
    "    df[\"clean_text\"] = df[\"text\"].map(_clean_safe)\n",
    "\n",
    "    # create_context_window(clean_text, candidate_string)\n",
    "    def _ctx_safe(row):\n",
    "        try:\n",
    "            return create_context_window(row[\"clean_text\"], row[\"candidate_string\"])\n",
    "        except Exception:\n",
    "            # Fallback: if something goes wrong, just return the clean_text\n",
    "            return row[\"clean_text\"]\n",
    "\n",
    "    df[\"modified_text\"] = df.apply(_ctx_safe, axis=1)\n",
    "\n",
    "    # --- 6) Build outputs ---\n",
    "    X_text = df[\"modified_text\"].astype(str).tolist()\n",
    "    X_candidate = df[\"candidate_string\"].astype(str).tolist()\n",
    "    Y_labels = df[\"label\"].tolist()\n",
    "\n",
    "    return X_text, X_candidate, Y_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LIRjSxkJp8u"
   },
   "source": [
    "## works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report,f1_score,precision_score,recall_score\n",
    "import numpy as np\n",
    "import os # For path joining\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "if torch.cuda.is_available(): # If using PyTorch\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Existing Strong Performers & Baselines ---\n",
    "# 1. RoBERTa (Robustly Optimized BERT Pretraining Approach)\n",
    "#MODEL_NAME    = \"roberta-base\"\n",
    "\n",
    "\n",
    "# 2,3. BERT (Bidirectional Encoder Representations from Transformers)\n",
    "#MODEL_NAME    = \"bert-base-cased\"\n",
    "#MODEL_NAME    = \"bert-base-uncased\"\n",
    "\n",
    "\n",
    "# 4. ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)\n",
    "#MODEL_NAME    = \"google/electra-base-discriminator\"\n",
    "\n",
    "\n",
    "# 5,6. DistilBERT (Distilled version of BERT)\n",
    "#MODEL_NAME    = \"distilbert-base-cased\"\n",
    "#MODEL_NAME    = \"distilbert-base-uncased\"\n",
    "\n",
    "# 7. CodeBERT (Pre-trained on code and natural language)\n",
    "#MODEL_NAME    = \"microsoft/codebert-base\"\n",
    "\n",
    "# --- Newer or More Specialized Models to Experiment With ---\n",
    "\n",
    "# 8. ALBERT (A Lite BERT for Self-supervised Learning of Language Representations)\n",
    "# - Parameter reduction techniques for lower memory consumption and faster training.\n",
    "# - Often performs well, good efficiency/performance trade-off.\n",
    "#MODEL_NAME    = \"albert-base-v2\"\n",
    "#Crashing idk why\n",
    "\n",
    "# 9. XLNet (Generalized Autoregressive Pretraining for Language Understanding)\n",
    "# - Uses a permutation language modeling objective, different from BERT's masked LM.\n",
    "# - Can capture bidirectional context well.\n",
    "#MODEL_NAME    = \"xlnet-base-cased\"\n",
    "\n",
    "\n",
    "# 10. BigBird (Sparse Attention for Longer Sequences) - Another option for long sequences\n",
    "# - Uses sparse attention mechanisms (block sparse attention) to handle longer inputs efficiently.\n",
    "# - Often a strong performer for tasks requiring understanding of long contexts.\n",
    "MODEL_NAME    = \"google/bigbird-roberta-base\" # Based on RoBERTa architecture\n",
    "#Crashing idk why\n",
    "\n",
    "\n",
    "# 11. Funnel-Transformer\n",
    "# - Gradually reduces the sequence length in deeper layers, focusing computation on higher-level representations.\n",
    "# - Can be more efficient.\n",
    "#MODEL_NAME    = \"funnel-transformer/medium\"\n",
    "\n",
    "\n",
    "# 12. LUKE (Language Understanding with Knowledge-based Embeddings)\n",
    "# - Enhances language models by incorporating entity embeddings and knowledge graph information.\n",
    "# - Could be interesting if your secrets are named entities or have known structures.\n",
    "#MODEL_NAME    = \"studio-ousia/luke-base\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading and Preprocessing (Placeholder) ---\n",
    "train_df = pd.read_csv(\"../Data/train.csv\")\n",
    "val_df = pd.read_csv(\"../Data/val.csv\")\n",
    "test_df = pd.read_csv(\"../Data/test.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DATASET_TYPE = \"balanced\"\n",
    "\n",
    "X_text_train,X_candidate_train,Y_labels_train = process_dataframe(train_df)\n",
    "X_text_val,X_candidate_val,Y_labels_val = process_dataframe(val_df)\n",
    "X_text_test,X_candidate_test,Y_labels_test = process_dataframe(test_df)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Train samples: {len(X_text_train)}\")\n",
    "print(f\"Validation samples: {len(X_text_val)}\")\n",
    "print(f\"Test samples: {len(X_text_test)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 1 # Reduced for quicker demonstration, increase for real training\n",
    "LEARNING_RATE = 2e-5 # Common starting point for fine-tuning transformers\n",
    "MAX_LENGTH = 256 # Reduced for potentially faster training, RoBERTa can handle 512\n",
    "BEST_MODEL_PATH = \"models/\"+DATASET_TYPE+\"/\"+\"best_\"+MODEL_NAME.replace(\"/\", \"_\")+\"_model.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For your specific task, it seems you want to encode text and candidate separately\n",
    "# and then pass them to a model that can handle two separate inputs.\n",
    "# However, RobertaForSequenceClassification expects either:\n",
    "# 1. A single sequence: tokenizer(text, truncation=True, padding=True)\n",
    "# 2. A pair of sequences: tokenizer(text1, text2, truncation=True, padding=True)\n",
    "#\n",
    "# Your original `CustomDataset` suggests you are passing two separate tokenized inputs.\n",
    "# This implies your model needs to be able to process them.\n",
    "# If you are using `RobertaForSequenceClassification` directly, it will interpret the *first*\n",
    "# set of input_ids and attention_mask as the primary input.\n",
    "#\n",
    "# Let's adjust to the common way of using `RobertaForSequenceClassification` for pairs:\n",
    "# Concatenate or pass as pair to tokenizer\n",
    "# Option A: Concatenate with [SEP]\n",
    "# X_combined_train = [text + \" [SEP] \" + cand for text, cand in zip(X_text_train, X_candidate_train)]\n",
    "# train_encodings = tokenizer(X_combined_train, padding=True, truncation=True, return_tensors='pt', max_length=MAX_LENGTH)\n",
    "\n",
    "# --- Tokenization ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# Option B: Pass as pair (preferred if model supports it well, like BERT, RoBERTa)\n",
    "print(\"Tokenizing training data...\")\n",
    "train_encodings = tokenizer(X_text_train, X_candidate_train, padding=True, truncation=True, return_tensors='pt', max_length=MAX_LENGTH)\n",
    "print(\"Tokenizing validation data...\")\n",
    "val_encodings = tokenizer(X_text_val, X_candidate_val, padding=True, truncation=True, return_tensors='pt', max_length=MAX_LENGTH)\n",
    "print(\"Tokenizing test data...\")\n",
    "test_encodings = tokenizer(X_text_test, X_candidate_test, padding=True, truncation=True, return_tensors='pt', max_length=MAX_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Convert labels to numpy arrays of integers\n",
    "Y_labels_train_arr = np.array(Y_labels_train).astype(int)\n",
    "Y_labels_val_arr = np.array(Y_labels_val).astype(int)\n",
    "Y_labels_test_arr = np.array(Y_labels_test).astype(int)\n",
    "\n",
    "train_dataset = PairDataset(train_encodings, Y_labels_train_arr)\n",
    "val_dataset = PairDataset(val_encodings, Y_labels_val_arr)\n",
    "test_dataset = PairDataset(test_encodings, Y_labels_test_arr)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model, Optimizer, Scheduler ---\n",
    "# Determine number of unique labels\n",
    "num_labels = len(np.unique(np.concatenate([Y_labels_train_arr, Y_labels_val_arr, Y_labels_test_arr])))\n",
    "print(f\"Number of unique labels: {num_labels}\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels,use_safetensors=True)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "num_training_steps = NUM_EPOCHS * len(train_loader)\n",
    "num_warmup_steps = int(0.1 * num_training_steps) # 10% warmup is common\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"models/\"+DATASET_TYPE)\n",
    "path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Define checkpoint directory and paths\n",
    "CKPT_DIR = Path(\"models/\" + DATASET_TYPE + \"/\" + MODEL_NAME.replace(\"/\", \"_\") + \"/checkpoints\")\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BEST_CKPT_PATH = CKPT_DIR / \"best.pt\"\n",
    "LATEST_CKPT_PATH = CKPT_DIR / \"latest.pt\"\n",
    "\n",
    "# Function to save checkpoint\n",
    "def save_checkpoint(path, model, optimizer, scheduler, epoch, global_step, best_metric):\n",
    "    state = {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict() if optimizer is not None else None,\n",
    "        \"scheduler_state_dict\": scheduler.state_dict() if scheduler is not None else None,\n",
    "        \"epoch\": epoch,\n",
    "        \"global_step\": global_step,\n",
    "        \"best_metric\": best_metric,\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "\n",
    "# Function to load checkpoint\n",
    "def load_checkpoint(path, model, optimizer=None, scheduler=None):\n",
    "    ckpt = torch.load(path, map_location=\"cpu\")\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    if optimizer is not None and ckpt.get(\"optimizer_state_dict\") is not None:\n",
    "        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
    "    if scheduler is not None and ckpt.get(\"scheduler_state_dict\") is not None:\n",
    "        scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
    "    return ckpt.get(\"epoch\", 0), ckpt.get(\"global_step\", 0), ckpt.get(\"best_metric\", -float(\"inf\"))\n",
    "\n",
    "# ---- If you want to resume automatically when 'latest.pt' exists ----\n",
    "resume = os.path.exists(LATEST_CKPT_PATH)\n",
    "start_epoch = 0\n",
    "global_step = 0\n",
    "best_val_f1_macro = -float(\"inf\")  # fix double-init bug\n",
    "\n",
    "if resume:\n",
    "    start_epoch, global_step, best_val_f1_macro = load_checkpoint(LATEST_CKPT_PATH, model, optimizer, scheduler)\n",
    "\n",
    "# --- Optional: also save periodic step checkpoints ---\n",
    "SAVE_EVERY_STEPS = 0  # set e.g. 500 to enable\n",
    "\n",
    "# --- Compute class weights for imbalanced dataset ---\n",
    "def compute_weights(train_loader):\n",
    "    all_labels = []\n",
    "    for batch in train_loader:\n",
    "        all_labels.extend(batch['labels'].cpu().numpy())\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(all_labels), y=all_labels)\n",
    "    return torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "# Get class weights for the dataset\n",
    "# class_weights = compute_weights(train_loader)\n",
    "# criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# --- Training and Validation Loop ---\n",
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\", leave=False)\n",
    "    for batch_idx, batch in enumerate(train_progress_bar):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        token_type_ids = batch.get('token_type_ids')\n",
    "        kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "        if token_type_ids is not None:\n",
    "            kwargs[\"token_type_ids\"] = token_type_ids.to(DEVICE)\n",
    "\n",
    "        outputs = model(**kwargs)\n",
    "        #loss = criterion(outputs.logits, labels)  # Use weighted loss here\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        global_step += 1\n",
    "        train_progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\", 'step': global_step})\n",
    "\n",
    "        # Periodic step checkpoint (optional)\n",
    "        if SAVE_EVERY_STEPS and (global_step % SAVE_EVERY_STEPS == 0):\n",
    "            step_path = CKPT_DIR / f\"step_{global_step}.pt\"\n",
    "            save_checkpoint(step_path, model, optimizer, scheduler, epoch, global_step, best_val_f1_macro)\n",
    "    \n",
    "    avg_train_loss = total_train_loss / max(1, len(train_loader))\n",
    "    print(f\"Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_val_preds, all_val_labels = [], []\n",
    "    val_progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1} Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_progress_bar:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            token_type_ids = batch.get('token_type_ids')\n",
    "            kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "            if token_type_ids is not None:\n",
    "                kwargs[\"token_type_ids\"] = token_type_ids.to(DEVICE)\n",
    "\n",
    "            outputs = model(**kwargs)\n",
    "            #loss = criterion(outputs.logits, labels)  # Use weighted loss for validation too\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            all_val_preds.extend(preds.cpu().numpy())\n",
    "            all_val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            val_progress_bar.set_postfix({'val_loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_val_loss = total_val_loss / max(1, len(val_loader))\n",
    "    print(f\"\\n--- Detailed Validation Metrics for Epoch {epoch + 1} ---\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    unique_labels_true, counts_true = np.unique(all_val_labels, return_counts=True)\n",
    "    print(f\"True label distribution in validation set: {dict(zip(unique_labels_true, counts_true))}\")\n",
    "    unique_labels_pred, counts_pred = np.unique(all_val_preds, return_counts=True)\n",
    "    print(f\"Predicted label distribution in validation set: {dict(zip(unique_labels_pred, counts_pred))}\")\n",
    "\n",
    "    labels_for_report = sorted(list(set(all_val_labels) | set(all_val_preds)))\n",
    "    print(\"\\nDetailed Classification Report (Validation):\")\n",
    "    print(classification_report(all_val_labels, all_val_preds, labels=labels_for_report, zero_division=0))\n",
    "\n",
    "    val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
    "    if num_labels == 2:\n",
    "        val_precision_binary, val_recall_binary, val_f1_binary, _ = precision_recall_fscore_support(\n",
    "            all_val_labels, all_val_preds, average='binary', pos_label=1, zero_division=0\n",
    "        )\n",
    "    val_precision_macro, val_recall_macro, val_f1_macro, _ = precision_recall_fscore_support(\n",
    "        all_val_labels, all_val_preds, average='macro', zero_division=0\n",
    "    )\n",
    "    val_precision_weighted, val_recall_weighted, val_f1_weighted, _ = precision_recall_fscore_support(\n",
    "        all_val_labels, all_val_preds, average='weighted', zero_division=0\n",
    "    )\n",
    "\n",
    "    print(f\"\\nOverall Validation Metrics (Epoch {epoch+1}):\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    if num_labels == 2:\n",
    "        print(f\"Validation F1-Score (Binary, for class 1): {val_f1_binary:.4f}\")\n",
    "        print(f\"Validation Precision (Binary, for class 1): {val_precision_binary:.4f}\")\n",
    "        print(f\"Validation Recall (Binary, for class 1): {val_recall_binary:.4f}\")\n",
    "    print(f\"Validation F1-Score (Macro): {val_f1_macro:.4f}\")\n",
    "    print(f\"Validation Precision (Macro): {val_precision_macro:.4f}\")\n",
    "    print(f\"Validation Recall (Macro): {val_recall_macro:.4f}\")\n",
    "    print(f\"Validation F1-Score (Weighted): {val_f1_weighted:.4f}\")\n",
    "    print(f\"Validation Precision (Weighted): {val_precision_weighted:.4f}\")\n",
    "    print(f\"Validation Recall (Weighted): {val_recall_weighted:.4f}\")\n",
    "\n",
    "    # Choose metric to optimize\n",
    "    current_metric_to_optimize = val_f1_macro  # or val_f1_binary, etc.\n",
    "\n",
    "    # --- Save latest checkpoint every epoch (for resume) ---\n",
    "    save_checkpoint(LATEST_CKPT_PATH, model, optimizer, scheduler, epoch, global_step, best_val_f1_macro)\n",
    "\n",
    "    # --- Save best model (by chosen metric) ---\n",
    "    if current_metric_to_optimize > best_val_f1_macro:\n",
    "        best_val_f1_macro = current_metric_to_optimize\n",
    "        save_checkpoint(BEST_CKPT_PATH, model, optimizer, scheduler, epoch, global_step, best_val_f1_macro)\n",
    "        # (Optional) also keep a copy at your existing BEST_MODEL_PATH if needed\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f\"ðŸŒŸ New best model saved to {BEST_CKPT_PATH} (metric: {best_val_f1_macro:.4f})\")\n",
    "    else:\n",
    "        print(f\"No improvement this epoch. Best metric so far: {best_val_f1_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "if os.path.exists(CKPT_DIR):\n",
    "    shutil.rmtree(CKPT_DIR)  # removes everything inside\n",
    "    print(f\"Removed: {CKPT_DIR}\")\n",
    "else:\n",
    "    print(\"Directory does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from typing import List, Optional, Union, Any\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    y_true: Union[np.ndarray, pd.Series, List[Any]],\n",
    "    y_pred: Union[np.ndarray, pd.Series, List[Any]],\n",
    "    classes: Optional[List[str]] = None,\n",
    "    normalize: bool = False,\n",
    "    title: str = 'Confusion Matrix',\n",
    "    cmap: str = plt.cm.Blues, # Colormap, e.g., Blues, Greens, YlOrRd\n",
    "    figsize: tuple = (8, 6),\n",
    "    fmt: Optional[str] = None, # Format for annotations (e.g., '.2f' for normalize)\n",
    "    print_raw_matrix: bool = False # Option to print the raw matrix to console\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Ground truth (correct) target values.\n",
    "        y_pred (array-like): Estimated targets as returned by a classifier.\n",
    "        classes (list of str, optional): List of names to map to classes (e.g., ['Class 0', 'Class 1']).\n",
    "                                         If None, integers 0, 1, 2... will be used.\n",
    "        normalize (bool, optional): Whether to normalize the confusion matrix. Defaults to False.\n",
    "        title (str, optional): Title for the plot. Defaults to 'Confusion Matrix'.\n",
    "        cmap (str or Colormap, optional): Matplotlib colormap. Defaults to plt.cm.Blues.\n",
    "        figsize (tuple, optional): Figure size (width, height) in inches. Defaults to (8, 6).\n",
    "        fmt (str, optional): String formatting code to use when adding annotations.\n",
    "                             Defaults to 'd' for integers, '.2f' if normalize=True.\n",
    "        print_raw_matrix (bool, optional): If True, prints the raw confusion matrix array to the console.\n",
    "                                          Defaults to False.\n",
    "    \"\"\"\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if print_raw_matrix:\n",
    "        print(\"Raw Confusion Matrix:\")\n",
    "        print(cm)\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    # Determine class labels for the plot\n",
    "    if classes is None:\n",
    "        # Infer classes from the unique values present in y_true and y_pred\n",
    "        inferred_classes = np.unique(np.concatenate((y_true, y_pred)))\n",
    "        tick_labels = [str(c) for c in inferred_classes]\n",
    "    else:\n",
    "        # Ensure the provided classes are used for plotting\n",
    "        # The confusion_matrix might have a different order or subset if labels argument isn't used\n",
    "        # For plotting, we use the provided 'classes' directly for tick labels.\n",
    "        # If `labels` argument was used in `confusion_matrix` call, cm will match this order.\n",
    "        # If not, and y_true/y_pred don't contain all classes, the plot might be misleading.\n",
    "        # It's best if 'classes' provided here match the 'labels' used to generate 'cm'.\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=classes if classes else None)\n",
    "        tick_labels = classes\n",
    "\n",
    "    # Handle normalization\n",
    "    if normalize:\n",
    "        # Calculate row sums, handle cases where sum is 0 to avoid division by zero\n",
    "        row_sums = cm.sum(axis=1)[:, np.newaxis]\n",
    "        # Replace 0s in row_sums with 1s to avoid division by zero, result will be 0 anyway for that row\n",
    "        safe_row_sums = np.where(row_sums == 0, 1, row_sums)\n",
    "        cm_normalized = cm.astype('float') / safe_row_sums\n",
    "        plot_data = cm_normalized\n",
    "        if fmt is None:\n",
    "            fmt = '.2f' # Default format for normalized values\n",
    "        title = f'{title} (Normalized)'\n",
    "    else:\n",
    "        plot_data = cm\n",
    "        if fmt is None:\n",
    "            fmt = 'd' # Default format for counts\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(\n",
    "        plot_data,\n",
    "        annot=True,       # Show numbers in cells\n",
    "        fmt=fmt,          # Format for the numbers\n",
    "        cmap=cmap,        # Colormap\n",
    "        xticklabels=tick_labels,\n",
    "        yticklabels=tick_labels,\n",
    "        linewidths=.5,    # Add lines between cells\n",
    "        cbar=True         # Show color bar\n",
    "    )\n",
    "\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.ylabel('True Label', fontsize=14)\n",
    "    plt.xlabel('Predicted Label', fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12, rotation=0) # Keep y-axis labels horizontal\n",
    "    plt.tight_layout() # Adjust plot to prevent labels from overlapping\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "model.to(DEVICE)  # Ensure model is on the correct device after loading\n",
    "model.eval()\n",
    "\n",
    "all_test_preds = []\n",
    "all_test_labels = []\n",
    "total_test_loss = 0\n",
    "\n",
    "# Use the same weighted loss function as during training\n",
    "#criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "test_progress_bar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "with torch.no_grad():\n",
    "    for batch in test_progress_bar:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        token_type_ids = batch.get('token_type_ids')\n",
    "        if token_type_ids is not None:\n",
    "            token_type_ids = token_type_ids.to(DEVICE)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "        else:\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        #loss = criterion(outputs.logits, labels)  # Use weighted loss here\n",
    "        loss = outputs.loss\n",
    "        total_test_loss += loss.item()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        all_test_preds.extend(predictions.cpu().numpy())\n",
    "        all_test_labels.extend(labels.cpu().numpy())\n",
    "        test_progress_bar.set_postfix({'test_loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "# Calculate overall test loss and accuracy\n",
    "avg_test_loss = total_test_loss / len(test_loader)\n",
    "test_accuracy = accuracy_score(all_test_labels, all_test_preds)\n",
    "\n",
    "# Print and save the metrics\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "\n",
    "# Check actual label distribution in test set\n",
    "unique_labels_true, counts_true = np.unique(all_test_labels, return_counts=True)\n",
    "print(f\"True label distribution in test set: {dict(zip(unique_labels_true, counts_true))}\")\n",
    "\n",
    "# Check predicted label distribution\n",
    "unique_labels_pred, counts_pred = np.unique(all_test_preds, return_counts=True)\n",
    "print(f\"Predicted label distribution in test set: {dict(zip(unique_labels_pred, counts_pred))}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report (Test):\")\n",
    "labels_for_report = sorted(list(set(all_test_labels) | set(all_test_preds)))\n",
    "print(classification_report(all_test_labels, all_test_preds, labels=labels_for_report, zero_division=0))\n",
    "classification_report = classification_report(all_test_labels, all_test_preds, labels=labels_for_report, zero_division=0)\n",
    "\n",
    "# Precision, Recall, F1-Score metrics\n",
    "test_precision_macro, test_recall_macro, test_f1_macro, _ = precision_recall_fscore_support(\n",
    "    all_test_labels, all_test_preds, average='macro', zero_division=0\n",
    ")\n",
    "test_precision_weighted, test_recall_weighted, test_f1_weighted, _ = precision_recall_fscore_support(\n",
    "    all_test_labels, all_test_preds, average='weighted', zero_division=0\n",
    ")\n",
    "test_f1 = f1_score(all_test_labels, all_test_preds)\n",
    "test_precision = precision_score(all_test_labels, all_test_preds)\n",
    "test_recall = recall_score(all_test_labels, all_test_preds)\n",
    "\n",
    "# Specify the file path for saving metrics\n",
    "output_file = \"models/\" + DATASET_TYPE + \"/\" + \"test_metrics.txt\"\n",
    "\n",
    "# Open the file in append mode\n",
    "with open(output_file, \"a\") as f:\n",
    "    f.write(\"\\nOverall Test Metrics (\" + MODEL_NAME + \")\\n\")\n",
    "    f.write(f\"Test Loss: {avg_test_loss:.4f}\\n\")\n",
    "    f.write(f\"Test Accuracy: {test_accuracy:.4f}\\n\")\n",
    "    f.write(f\"Test F1-Score: {test_f1:.4f}\\n\")\n",
    "    f.write(f\"Test Precision: {test_precision:.4f}\\n\")\n",
    "    f.write(f\"Test Recall: {test_recall:.4f}\\n\")\n",
    "    f.write(f\"Test F1-Score (Macro): {test_f1_macro:.4f}\\n\")\n",
    "    f.write(f\"Test Precision (Macro): {test_precision_macro:.4f}\\n\")\n",
    "    f.write(f\"Test Recall (Macro): {test_recall_macro:.4f}\\n\")\n",
    "    f.write(f\"Test F1-Score (Weighted): {test_f1_weighted:.4f}\\n\")\n",
    "    f.write(f\"Test Precision (Weighted): {test_precision_weighted:.4f}\\n\")\n",
    "    f.write(f\"Test Recall (Weighted): {test_recall_weighted:.4f}\\n\")\n",
    "    f.write(f\"Detailed Classification Report (Test):\\n {classification_report}\\n\")\n",
    "\n",
    "print(f\"\\nMetrics have been saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1707329281053,
     "user": {
      "displayName": "Sadif Ahmed",
      "userId": "17613161859460678714"
     },
     "user_tz": -360
    },
    "id": "GdrXjdHzbN8R"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(all_test_labels,all_test_preds)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
