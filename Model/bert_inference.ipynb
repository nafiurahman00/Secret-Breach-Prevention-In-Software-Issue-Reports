{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "RANDOM_SEED = 42 # Your chosen seed\n",
    "\n",
    "def set_all_seeds(seed_value):\n",
    "    print(f\"Setting all seeds to: {seed_value}\")\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value) # Set it for consistent hashing\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed_value) # for multi-GPU.\n",
    "        torch.cuda.manual_seed(seed_value)     # for current GPU.\n",
    "\n",
    "# Call this AT THE VERY BEGINNING of your script, before almost any other import or operation\n",
    "set_all_seeds(RANDOM_SEED)\n",
    "\n",
    "# ... then your other imports like pandas, tqdm, sklearn, transformers ...\n",
    "# ... then your model definition, tokenizer, dataset, dataloader creation ...\n",
    "# ... then your training loop ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context Window Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1707329188201,
     "user": {
      "displayName": "Sadif Ahmed",
      "userId": "17613161859460678714"
     },
     "user_tz": -360
    },
    "id": "JjYKTmpsEbWp",
    "outputId": "763d1d86-a823-4f56-d720-fc85881c5ea7"
   },
   "outputs": [],
   "source": [
    "def create_context_window(text, target_string, window_size=200):\n",
    "\n",
    "    target_index = text.find(target_string)\n",
    "\n",
    "    if target_index != -1:\n",
    "        start_index = max(0, target_index - window_size)\n",
    "        end_index = min(len(text), target_index + len(target_string) + window_size)\n",
    "        context_window = text[start_index:end_index]\n",
    "        return context_window\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_non_printable=True):\n",
    "    \"\"\"\n",
    "    Clean a single text string for BART fine-tuning.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): The input text.\n",
    "    - remove_non_printable (bool): Whether to remove non-printable characters.\n",
    "    \n",
    "    Returns:\n",
    "    - str: Cleaned text.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Strip leading/trailing whitespace\n",
    "    cleaned = text.strip()\n",
    "    \n",
    "    # 2. Replace multiple newlines/tabs with a space\n",
    "    cleaned = re.sub(r'[\\r\\n\\t]+', ' ', cleaned)\n",
    "    \n",
    "    # 3. Remove excessive spaces\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    \n",
    "    # 4. Optionally remove embedded <eos> or </s> tokens (Bart uses </s> as EOS)\n",
    "    cleaned = re.sub(r'(</s>|<eos>)', '', cleaned)\n",
    "    \n",
    "    \n",
    "    # 5. Optionally remove non-printable characters\n",
    "    if remove_non_printable:\n",
    "        printable_chars = set(string.printable)\n",
    "        cleaned = ''.join(filter(lambda x: x in printable_chars, cleaned))\n",
    "        \n",
    "    cleaned = re.sub(r'[\\'\"\\│]', '', cleaned)\n",
    "    dir_list_clean = re.sub(r'drwx[-\\s]*\\d+\\s+\\w+\\s+\\w+\\s+\\d+\\s+\\w+\\s+\\d+\\s+[0-9a-fA-F-]+.*','',cleaned)\n",
    "    shell_code_free_text = re.sub(r'```shell([^`]+)```','',dir_list_clean,flags=re.IGNORECASE)\n",
    "    shell_code_free_text = re.sub(r'```Shell\\s*\"([^\"]*)\"\\s*```','',shell_code_free_text,flags=re.IGNORECASE)\n",
    "    # saved_game_free_text = re.sub(r'```([^`]+)```','',shell_code_free_text) #etay jhamela hobe\n",
    "    saved_game_free_text = re.sub(r'<details><summary>Saved game</summary>\\n\\n```(.*?)```', '', shell_code_free_text)\n",
    "    remove_packages = re.sub(r'(\\w+\\.)+\\w+','',saved_game_free_text)\n",
    "    java_exp_free_text = re.sub(r'at\\s[\\w.$]+\\.([\\w]+)\\(([^:]+:\\d+)\\)','',remove_packages)\n",
    "    # url_free_text= re.sub(https?://[^\\s#]+#[A-Za-z0-9\\-]+,'', java_exp_free_text, flags=re.IGNORECASE)\n",
    "    url_with_fragment_text= re.sub(r'https?://[^\\s#]+#[A-Za-z0-9\\-\\=\\+]+','', java_exp_free_text, flags=re.IGNORECASE)\n",
    "    url_free_text= re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '',url_with_fragment_text)\n",
    "    commit_free_text= re.sub(r'commit[ ]?(?:id)?[ ]?[:]?[ ]?([0-9a-f]{40})\\b', '', url_free_text, flags=re.IGNORECASE)\n",
    "    file_path_free_text = re.sub(r\"/[\\w/. :-]+\",'',commit_free_text)\n",
    "    file_path_free_text = re.sub( r'(/[^/\\s]+)+','',file_path_free_text)\n",
    "    sha256_free_text = re.sub(r'sha256\\s*[:]?[=]?\\s*[a-fA-F0-9]{64}','',file_path_free_text)\n",
    "    sha1_free_text = re.sub(r'git-tree-sha1\\s*=\\s*[a-fA-F0-9]+','',sha256_free_text)\n",
    "    build_id_free_text = re.sub(r'build-id\\s*[:]?[=]?\\s*([a-fA-F0-9]+)','',sha1_free_text)\n",
    "    guids_free_text = re.sub(r'GUIDs:\\s+([0-9a-fA-F-]+\\s+[0-9a-fA-F-]+\\s+[0-9a-fA-F-]+)','',build_id_free_text)\n",
    "    uuids_free_text = re.sub(r'([0-9a-fA-F-]+\\s*,\\s*[0-9a-fA-F-]+\\s*,\\s*[0-9a-fA-F-]+)','',guids_free_text)\n",
    "    event_id_free_text = re.sub(r'<([^>]+)>','',uuids_free_text)\n",
    "    UUID_free_text = re.sub(r'(?:UUID|GUID|version|id)[\\\\=:\"\\'\\s]*\\b[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}\\b'\n",
    ",'',event_id_free_text,flags=re.IGNORECASE) ##without the prefix so many false positives can be omitted\n",
    "    hex_free_text = re.sub(r'(?:data|address|id)[\\\\=:\"\\'\\s]*\\b0x[0-9a-fA-F]+\\b','',UUID_free_text,flags=re.IGNORECASE) ## deleting hex ids directly can cause issues\n",
    "    ss_free_text = re.sub(r'Screenshot_(\\d{4}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\w+)','',hex_free_text,flags=re.IGNORECASE)\n",
    "    cleaned = ss_free_text    \n",
    "\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enable tqdm integration with pandas apply\n",
    "tqdm.pandas()\n",
    "\n",
    "def process_dataframe(input_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Processes an input Pandas DataFrame and returns a new, modified DataFrame,\n",
    "    along with lists of modified text and candidate strings.\n",
    "\n",
    "    Args:\n",
    "        input_df (pd.DataFrame): The input Pandas DataFrame to be processed.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (processed DataFrame, list of modified text, list of candidate strings)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1. Input Validation ---\n",
    "    if not isinstance(input_df, pd.DataFrame):\n",
    "        raise TypeError(\"Input must be a Pandas DataFrame.\")\n",
    "    if input_df.empty:\n",
    "        print(\"Warning: Input DataFrame is empty.\")\n",
    "        return input_df.copy(), [], []\n",
    "\n",
    "    # --- 2. Processing ---\n",
    "    preprocessed_df = input_df.copy()\n",
    "\n",
    "\n",
    "\n",
    "    # Apply `create_context_window` with progress bar\n",
    "    print(\"Creating context windows...\")\n",
    "    preprocessed_df['modified_text'] = preprocessed_df.progress_apply(\n",
    "        lambda row: create_context_window(row['text'], row['candidate_string']), axis=1)\n",
    "\n",
    "    # Extract lists\n",
    "    X_text = preprocessed_df['modified_text'].tolist()\n",
    "    X_candidate = preprocessed_df['candidate_string'].tolist()\n",
    "    \n",
    "    X_text = [str(x) for x in X_text]\n",
    "    X_candidate = [str(x) for x in X_candidate]\n",
    "\n",
    "\n",
    "    return preprocessed_df, X_text, X_candidate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_candidate_strings(regex_excel_path, issue_df):\n",
    "    \"\"\"\n",
    "    Extracts candidate strings matching secret-detection regex patterns from issue text.\n",
    "\n",
    "    Parameters:\n",
    "        regex_excel_path (str): Path to the Excel file with regex patterns.\n",
    "        issue_df (pd.DataFrame): DataFrame containing at least 'Issue ID' and 'Issue Body'.\n",
    "        output_csv_path (str): Path where the output CSV will be saved.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame of unique (Issue ID, Candidate String, Type) matches.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "     # Apply `clean_text` with progress bar\n",
    "    print(\"Cleaning text...\")\n",
    "    issue_df['Issue Body'] = issue_df.progress_apply(\n",
    "        lambda row: clean_text(row['Issue Body']), axis=1)\n",
    "    \n",
    "    \n",
    "    # Load regex patterns\n",
    "    excel_data = pd.read_excel(regex_excel_path)\n",
    "    regex = pd.DataFrame(excel_data, columns=['Pattern_ID', 'Secret Type', 'Regular Expression', 'Source'])\n",
    "\n",
    "    data_dict = {}\n",
    "    idx = 0\n",
    "\n",
    "    # Loop through each regex pattern with progress bar\n",
    "    for i in tqdm(regex.index, desc=\"Processing regex patterns\"):\n",
    "        pattern = re.compile(regex.at[i, 'Regular Expression'])\n",
    "\n",
    "        # Loop through each issue\n",
    "        for j in issue_df.index:\n",
    "            cleaned_text = issue_df.at[j, 'Issue Body']\n",
    "            matches = re.findall(pattern, cleaned_text)\n",
    "\n",
    "            for match in set(matches):\n",
    "                data_dict[idx] = {\n",
    "                    'Type': regex.at[i, 'Secret Type'],\n",
    "                    'Issue ID': issue_df.at[j, 'Issue ID'],\n",
    "                    'Candidate String': match\n",
    "                }\n",
    "                idx += 1\n",
    "\n",
    "    # Convert to DataFrame and drop duplicates\n",
    "    result_df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "    result_df = result_df.drop_duplicates(subset=[\"Issue ID\", \"Candidate String\"], keep='first')\n",
    "\n",
    "    print(f\"Extracted {result_df.shape[0]} unique matches.\")\n",
    "    return result_df,issue_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LIRjSxkJp8u"
   },
   "source": [
    "## works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report,f1_score,precision_score,recall_score\n",
    "import numpy as np\n",
    "import os # For path joining\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "if torch.cuda.is_available(): # If using PyTorch\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Existing Strong Performers & Baselines ---\n",
    "# 1. RoBERTa (Robustly Optimized BERT Pretraining Approach)\n",
    "MODEL_NAME    = \"roberta-base\"\n",
    "\n",
    "# 6. CodeBERT (Pre-trained on code and natural language)\n",
    "#MODEL_NAME    = \"microsoft/codebert-base\"\n",
    "\n",
    "\n",
    "# 14. LUKE (Language Understanding with Knowledge-based Embeddings)\n",
    "# - Enhances language models by incorporating entity embeddings and knowledge graph information.\n",
    "# - Could be interesting if your secrets are named entities or have known structures.\n",
    "#MODEL_NAME    = \"studio-ousia/luke-base\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "DATASET_TYPE = \"balanced\"\n",
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 20 # Reduced for quicker demonstration, increase for real training\n",
    "LEARNING_RATE = 2e-5 # Common starting point for fine-tuning transformers\n",
    "MAX_LENGTH = 256 # Reduced for potentially faster training, RoBERTa can handle 512\n",
    "BEST_MODEL_PATH = \"models/\"+DATASET_TYPE+\"/\"+\"best_\"+MODEL_NAME.replace(\"/\", \"_\")+\"_model.pth\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Example input\n",
    "# data = {\n",
    "#     \"1\": {\n",
    "#         \"issue_body\": \"\"\" I'm getting a permission error when trying to upload to our S3 bucket using the boto3 client. Here’s a snippet of the config I’m using:\n",
    "# ```pyth\n",
    "# on\n",
    "# aws_access_key_id = \"AKIAIOSFODNN7EXAMPLE\"\n",
    "# aws_secret_access_key = sk_67y453tc6hz129uz\"\"\",\n",
    "#         \"candidates\": \"sk_67y453tc6hz129uz\"\n",
    "#     },\n",
    "#     \"2\": {\n",
    "#         \"issue_body\": \"\"\"I'm getting a permission error when trying to upload to our S3 bucket using the boto3 client. Here’s a snippet of the config I’m using:\n",
    "# ```pyth\n",
    "# on\n",
    "# aws_access_key_id = \"AKIAIOSFODNN7EXAMPLE\"\n",
    "# # dummy key\n",
    "# aws_secret_access_key = sk_67y453tc6hz129uz\"\"\",\n",
    "#         \"candidates\": \"sk_67y453tc6hz129uz\",\n",
    "#     },\n",
    "#     \"3\": {\n",
    "#         \"issue_body\": \"\"\"I'm getting a permission error when trying to upload to our S3 bucket using the boto3 client. Here’s a snippet of the config I’m using:\n",
    "# ```pyth\n",
    "# on\n",
    "# aws_access_key_id = \"AKIAIOSFODNN7EXAMPLE\"\n",
    "# aws_secret_access_key = XXXXXXXX\"\"\",\n",
    "#         \"candidates\": \" XXXXXXXX\",\n",
    "#     }\n",
    "#     ,\n",
    "#     \"4\": {\n",
    "#         \"issue_body\": \"\"\" I'm getting a permission error when trying to upload to our S3 bucket using the boto3 client. Here’s a snippet of the config I’m using:\n",
    "# ```pyth\n",
    "# on\n",
    "# aws_access_key_id = \"AKIAIOSFODNN7EXAMPLE\"\n",
    "# aws_secret_access_key = DUMMY_KEY\"\"\",\n",
    "#         \"candidates\": \"DUMMY_KEY\",\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Convert dictionary to a dataset (flatten the structure)\n",
    "# dataset = []\n",
    "# for issue_id, issue_info in data.items():\n",
    "#     issue_body = issue_info[\"issue_body\"]\n",
    "#     candidate = issue_info[\"candidates\"]\n",
    "#     dataset.append({\n",
    "#             \"Issue ID\": issue_id,\n",
    "#             \"text\": issue_body,\n",
    "#             \"candidate_string\": candidate\n",
    "#         })\n",
    "\n",
    "# # Create a DataFrame\n",
    "# test_df = pd.DataFrame(dataset)\n",
    "\n",
    "# # Print or save the dataset\n",
    "# print(test_df)\n",
    "# # df.to_csv(\"issue_dataset.csv\", index=False)  # Uncomment to save to CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"Real_life_Issue_Reports_Merged.csv\")\n",
    "# data,df = extract_candidate_strings(\n",
    "#     regex_excel_path='../dataset/Secret-Regular-Expression.xlsx',\n",
    "#     issue_df=df\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.rename(columns={'Issue ID': 'Issue_id'})\n",
    "# print(data.shape)\n",
    "# print(data.head())\n",
    "# merged_df = df.merge(data, left_on=['Issue ID'], right_on=['Issue_id'])\n",
    "# print(merged_df.shape)\n",
    "# columns_to_remove = ['Issue_id']\n",
    "# merged_df.drop(columns=columns_to_remove, inplace=True)\n",
    "# print(merged_df.columns)\n",
    "\n",
    "# # Rename columns\n",
    "# merged_df = merged_df.rename(columns={\n",
    "#     'Issue Body': 'text',\n",
    "#     'Candidate String': 'candidate_string'\n",
    "# })\n",
    "# print(merged_df.columns)\n",
    "# print(merged_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df.to_csv('merged_issues-with-candidate-strings.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('merged_issues-with-candidate-strings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessed_df,X_text_test,X_candidate_test = process_dataframe(merged_df)\n",
    "\n",
    "print(f\"Test samples: {len(X_text_test)}\")\n",
    "\n",
    "# --- Tokenization ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For your specific task, it seems you want to encode text and candidate separately\n",
    "# and then pass them to a model that can handle two separate inputs.\n",
    "# However, RobertaForSequenceClassification expects either:\n",
    "# 1. A single sequence: tokenizer(text, truncation=True, padding=True)\n",
    "# 2. A pair of sequences: tokenizer(text1, text2, truncation=True, padding=True)\n",
    "#\n",
    "# Your original `CustomDataset` suggests you are passing two separate tokenized inputs.\n",
    "# This implies your model needs to be able to process them.\n",
    "# If you are using `RobertaForSequenceClassification` directly, it will interpret the *first*\n",
    "# set of input_ids and attention_mask as the primary input.\n",
    "#\n",
    "# Let's adjust to the common way of using `RobertaForSequenceClassification` for pairs:\n",
    "# Concatenate or pass as pair to tokenizer\n",
    "# Option A: Concatenate with [SEP]\n",
    "# X_combined_train = [text + \" [SEP] \" + cand for text, cand in zip(X_text_train, X_candidate_train)]\n",
    "# train_encodings = tokenizer(X_combined_train, padding=True, truncation=True, return_tensors='pt', max_length=MAX_LENGTH)\n",
    "\n",
    "# Option B: Pass as pair (preferred if model supports it well, like BERT, RoBERTa)\n",
    "print(\"Tokenizing test data...\")\n",
    "test_encodings = tokenizer(X_text_test, X_candidate_test, padding=True, truncation=True, return_tensors='pt', max_length=MAX_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, encodings, labels, indices=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.indices = indices if indices is not None else list(range(len(labels)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n",
    "        item['index'] = torch.tensor(self.indices[idx])  # <-- Add row index\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# Convert labels to numpy arrays of integers\n",
    "\n",
    "Y_labels_test_arr = np.zeros(len(X_text_test))\n",
    "\n",
    "# Save indices to map predictions back to preprocessed_df\n",
    "test_indices = list(preprocessed_df.index)\n",
    "\n",
    "test_dataset = PairDataset(test_encodings, Y_labels_test_arr, indices=test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model, Optimizer, Scheduler ---\n",
    "num_labels=2\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "model.to(DEVICE) # Ensure model is on the correct device after loading\n",
    "model.eval()\n",
    "\n",
    "all_test_preds = []\n",
    "all_test_labels = []\n",
    "all_indices = []\n",
    "total_test_loss = 0\n",
    "\n",
    "test_progress_bar = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "with torch.no_grad():\n",
    "    for batch in test_progress_bar:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        token_type_ids = batch.get('token_type_ids')\n",
    "        if token_type_ids is not None:\n",
    "            token_type_ids = token_type_ids.to(DEVICE)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "        else:\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_test_loss += loss.item()\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        all_test_preds.extend(predictions.cpu().numpy())\n",
    "        all_test_labels.extend(labels.cpu().numpy())\n",
    "        all_indices.extend(batch['index'].cpu().numpy())\n",
    "        test_progress_bar.set_postfix({'test_loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "# Convert dictionary to a dataset (flatten the structure)\n",
    "dataset = []\n",
    "for idx, pred in zip(all_indices, all_test_preds):\n",
    "    if pred == 1:\n",
    "        dataset.append({\n",
    "            \"Issue ID\": preprocessed_df.loc[idx, \"Issue ID\"],\n",
    "            \"text\": preprocessed_df.loc[idx, \"text\"],\n",
    "            \"candidate_string\": preprocessed_df.loc[idx, \"candidate_string\"]\n",
    "        })\n",
    "# Create a DataFrame\n",
    "flagged_df = pd.DataFrame(dataset)\n",
    "\n",
    "flagged_file_name = MODEL_NAME.replace(\"/\", \"_\") +\"_\"+str(all_test_preds.count(1))+\"_flagged.csv\"\n",
    "flagged_df.to_csv(flagged_file_name, index=False)  # Uncomment to save to CSV\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
