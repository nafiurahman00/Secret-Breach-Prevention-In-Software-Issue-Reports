{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.metrics\n",
    "import re\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import hashlib\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate String Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Issue ID                                         Issue Body\n",
      "0  2032018852                   Changed \"ar\" to \"are\" in readme.\n",
      "1  2036967098  ### Describe the Bug\\r\\n\\r\\numami behaving lik...\n",
      "2  2039165352  Closes #800, #2423, #2400, #2386, #2383, #2358...\n",
      "3  2041048669  ### Describe the Bug\\r\\n\\r\\nImpossible to load...\n",
      "4  2048399912  ### Describe the feature or enhancement\\r\\n\\r\\...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "df = pd.read_csv(\"crawled_issue/data.csv\")\n",
    "df_unique = df.drop_duplicates(subset='Issue ID', keep='first')\n",
    "df = df_unique\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install openpyxl \n",
    "excel_data = pd.read_excel('dataset/Secret-Regular-Expression.xlsx')\n",
    "\n",
    "# Read the values of the file in the dataframe\n",
    "regex = pd.DataFrame(excel_data, columns=[\n",
    "'Pattern_ID','Secret Type',\t'Regular Expression','Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue ID</th>\n",
       "      <th>Issue Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2032018852</td>\n",
       "      <td>Changed ar to are in readme.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2036967098</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\numami behaving lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2039165352</td>\n",
       "      <td>Closes #800, #2423, #2400, #2386, #2383, #2358...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2041048669</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\nImpossible to load...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2048399912</td>\n",
       "      <td>### Describe the feature or enhancement\\r\\n\\r\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2053026441</td>\n",
       "      <td>fixed #2433  bug please review\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2053308325</td>\n",
       "      <td>This PR includes updates to the Swedish transl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2054244873</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\n1. Start onboardin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2056044123</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\ni want to track re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2058073803</td>\n",
       "      <td>This PR corrects a spelling error in the Share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2064913595</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\nInside of the webs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2066413072</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\nAbout three weeks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2067476483</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\nI have both [Verce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2069253235</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\nUsing Umami Cloud ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2072231190</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\n![image](\\r\\n\\r\\nB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2074313481</td>\n",
       "      <td>### Describe the feature or enhancement\\r\\n\\r\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2075188399</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\nHello everyone,\\r\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2075567089</td>\n",
       "      <td>Bumps [follow-redirects]( from  to .\\r\\n\\r\\nCo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2082537025</td>\n",
       "      <td>fix pgbouncer to supavisor migration issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2083725701</td>\n",
       "      <td>This pull requests implements logging of the I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2091740149</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\nHi! Im attempting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2092019139</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\nWhen running Umami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2093753035</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\nI have been mighty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2107214385</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\nthe documentation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2115485810</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\nWe upgraded from  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2116188130</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\n![image](\\r\\n![ima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2117280225</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\nWhen logging in us...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Issue ID                                         Issue Body\n",
       "0   2032018852                       Changed ar to are in readme.\n",
       "1   2036967098  ### Describe the Bug\\r\\n\\r\\numami behaving lik...\n",
       "2   2039165352  Closes #800, #2423, #2400, #2386, #2383, #2358...\n",
       "3   2041048669  ### Describe the Bug\\r\\n\\r\\nImpossible to load...\n",
       "4   2048399912  ### Describe the feature or enhancement\\r\\n\\r\\...\n",
       "5   2053026441                 fixed #2433  bug please review\\r\\n\n",
       "6   2053308325  This PR includes updates to the Swedish transl...\n",
       "7   2054244873  ### Describe the Bug\\r\\n\\r\\n1. Start onboardin...\n",
       "8   2056044123  ### Describe the Bug\\r\\n\\r\\ni want to track re...\n",
       "9   2058073803  This PR corrects a spelling error in the Share...\n",
       "10  2064913595  ### Describe the Bug\\r\\n\\r\\nInside of the webs...\n",
       "11  2066413072  ### Describe the Bug\\r\\n\\r\\nAbout three weeks ...\n",
       "12  2067476483  ### Describe the Bug\\r\\n\\r\\nI have both [Verce...\n",
       "13  2069253235  ### Describe the Bug\\r\\n\\r\\nUsing Umami Cloud ...\n",
       "14  2072231190  ### Describe the Bug\\r\\n\\r\\n![image](\\r\\n\\r\\nB...\n",
       "15  2074313481  ### Describe the feature or enhancement\\r\\n\\r\\...\n",
       "16  2075188399  ### Describe the Bug\\r\\n\\r\\nHello everyone,\\r\\...\n",
       "17  2075567089  Bumps [follow-redirects]( from  to .\\r\\n\\r\\nCo...\n",
       "18  2082537025         fix pgbouncer to supavisor migration issue\n",
       "19  2083725701  This pull requests implements logging of the I...\n",
       "20  2091740149  ### Describe the Bug\\r\\n\\r\\nHi! Im attempting ...\n",
       "21  2092019139  ### Describe the Bug\\r\\n\\r\\nWhen running Umami...\n",
       "22  2093753035  ### Describe the Bug\\r\\n\\r\\nI have been mighty...\n",
       "23  2107214385  ### Describe the Bug\\r\\n\\r\\nthe documentation ...\n",
       "24  2115485810  ### Describe the Bug\\r\\n\\r\\nWe upgraded from  ...\n",
       "25  2116188130  ### Describe the Bug\\r\\n\\r\\n![image](\\r\\n![ima...\n",
       "26  2117280225  ### Describe the Bug\\r\\n\\r\\nWhen logging in us..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_dict={}\n",
    "for j in df.index:\n",
    "        # if df[\"id\"][j] != \"1165939311\":\n",
    "        #         continue\n",
    "        input_string =    str(df[\"Issue Body\"][j])    \n",
    "        input_string = re.sub(r'[\\'\"\\â”‚]', '', input_string)\n",
    "        dir_list_clean = re.sub(r'drwx[-\\s]*\\d+\\s+\\w+\\s+\\w+\\s+\\d+\\s+\\w+\\s+\\d+\\s+[0-9a-fA-F-]+.*','',input_string)\n",
    "        shell_code_free_text = re.sub(r'```shell([^`]+)```','',dir_list_clean,flags=re.IGNORECASE)\n",
    "        shell_code_free_text = re.sub(r'```Shell\\s*\"([^\"]*)\"\\s*```','',shell_code_free_text,flags=re.IGNORECASE)\n",
    "        # saved_game_free_text = re.sub(r'```([^`]+)```','',shell_code_free_text) #etay jhamela hobe\n",
    "        saved_game_free_text = re.sub(r'<details><summary>Saved game</summary>\\n\\n```(.*?)```', '', shell_code_free_text)\n",
    "        remove_packages = re.sub(r'(\\w+\\.)+\\w+','',saved_game_free_text)\n",
    "        java_exp_free_text = re.sub(r'at\\s[\\w.$]+\\.([\\w]+)\\(([^:]+:\\d+)\\)','',remove_packages)\n",
    "        # url_free_text= re.sub(https?://[^\\s#]+#[A-Za-z0-9\\-]+,'', java_exp_free_text, flags=re.IGNORECASE)\n",
    "        url_with_fragment_text= re.sub(r'https?://[^\\s#]+#[A-Za-z0-9\\-\\=\\+]+','', java_exp_free_text, flags=re.IGNORECASE)\n",
    "        url_free_text= re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '',url_with_fragment_text)\n",
    "        commit_free_text= re.sub(r'commit[ ]?(?:id)?[ ]?[:]?[ ]?([0-9a-f]{40})\\b', '', url_free_text, flags=re.IGNORECASE)\n",
    "        file_path_free_text = re.sub(r\"/[\\w/. :-]+\",'',commit_free_text)\n",
    "        file_path_free_text = re.sub( r'(/[^/\\s]+)+','',file_path_free_text)\n",
    "        sha256_free_text = re.sub(r'sha256\\s*[:]?[=]?\\s*[a-fA-F0-9]{64}','',file_path_free_text)\n",
    "        sha1_free_text = re.sub(r'git-tree-sha1\\s*=\\s*[a-fA-F0-9]+','',sha256_free_text)\n",
    "        build_id_free_text = re.sub(r'build-id\\s*[:]?[=]?\\s*([a-fA-F0-9]+)','',sha1_free_text)\n",
    "        guids_free_text = re.sub(r'GUIDs:\\s+([0-9a-fA-F-]+\\s+[0-9a-fA-F-]+\\s+[0-9a-fA-F-]+)','',build_id_free_text)\n",
    "        uuids_free_text = re.sub(r'([0-9a-fA-F-]+\\s*,\\s*[0-9a-fA-F-]+\\s*,\\s*[0-9a-fA-F-]+)','',guids_free_text)\n",
    "        event_id_free_text = re.sub(r'<([^>]+)>','',uuids_free_text)\n",
    "        UUID_free_text = re.sub(r'(?:UUID|GUID|version|id)[\\\\=:\"\\'\\s]*\\b[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}\\b'\n",
    ",'',event_id_free_text,flags=re.IGNORECASE) ##without the prefix so many false positives can be omitted\n",
    "        hex_free_text = re.sub(r'(?:data|address|id)[\\\\=:\"\\'\\s]*\\b0x[0-9a-fA-F]+\\b','',UUID_free_text,flags=re.IGNORECASE) ## deleting hex ids directly can cause issues\n",
    "        ss_free_text = re.sub(r'Screenshot_(\\d{4}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\w+)','',hex_free_text,flags=re.IGNORECASE)\n",
    "        cleaned_text = ss_free_text\n",
    "        # file_path = \"output.txt\"\n",
    "\n",
    "        # with open(file_path, 'w') as file:\n",
    "        #                 file.write(cleaned_text)\n",
    "        data_dict[j] = {'Issue ID':df['Issue ID'][j],'Issue Body':cleaned_text}\n",
    "        # idx = idx+1\n",
    "    \n",
    "\n",
    "\n",
    "cleaned_text_data = pd.DataFrame.from_dict(data_dict, \"index\")\n",
    "cleaned_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_17372\\2071773696.py:11: FutureWarning: Possible nested set at position 35\n",
      "  p = re.compile(regex['Regular Expression'][i])\n",
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_17372\\2071773696.py:11: DeprecationWarning: Flags not at the start of the expression '/(?i)-----\\\\s*?BEGIN[' (truncated) but at position 1\n",
      "  p = re.compile(regex['Regular Expression'][i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 3)\n"
     ]
    }
   ],
   "source": [
    "###recovered\n",
    "    \n",
    "idx = 0\n",
    "data_dict={}\n",
    "# start = iter*100000\n",
    "# end = (iter+1)*100000\n",
    "for i in regex.index:\n",
    "    #print(i,regex['Secret Type'][i]) #, regex['Regular Expression'][i])\n",
    "    # if i%100==0:\n",
    "    #     print(\"checkpoint\")\n",
    "    p = re.compile(regex['Regular Expression'][i])\n",
    "    \n",
    "    # print(\"=====================================================================\")\n",
    "    \n",
    "    for j in df.index:\n",
    "        \n",
    "        cleaned_text = cleaned_text_data.loc[j, 'Issue Body']\n",
    "            # Now you can use 'cleaned_text' for further processing\n",
    "       \n",
    "        matches = re.findall(p,cleaned_text)\n",
    "        for match in set(matches):\n",
    "                data_dict[idx] = {'Type': regex['Secret Type'][i], 'Issue ID':df['Issue ID'][j],'Candidate String':match} #,'Entropy':shannon_entropy(match)}\n",
    "                idx = idx+1\n",
    "    \n",
    "\n",
    "\n",
    "data = pd.DataFrame.from_dict(data_dict, \"index\")\n",
    "data=data.drop_duplicates(subset=[\"Issue ID\", \"Candidate String\"], keep='first')\n",
    "print(data.shape)\n",
    "data.to_csv('crawled_issue/issues-with-candidate-strings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context Window and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 3)\n",
      "           Type    Issue_id                      Candidate String\n",
      "0  Anypoint API  2115485810  5509fd77-5a93-5372-b0ec-223d33ccbf76\n",
      "1  Anypoint API  2115485810  e24894c3-76d1-582a-a1ff-7becec3c8c3d\n",
      "2  Anypoint API  2115485810  459a7bb0-e9ed-5cc2-a285-4d1eca9a7fb9\n",
      "3  Anypoint API  2115485810  7d66612e-3661-5fc7-aad4-f2b7e8e28bd5\n",
      "4  Anypoint API  2115485810  1cd4f507-d10f-5449-be64-8c324d5a87a1\n",
      "(23, 5)\n",
      "Index(['Issue ID', 'Issue Body', 'Type', 'Candidate String'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = data.rename(columns={'Issue ID': 'Issue_id'})\n",
    "print(data.shape)\n",
    "print(data.head())\n",
    "merged_df = df.merge(data, left_on='Issue ID', right_on='Issue_id')\n",
    "print(merged_df.shape)\n",
    "columns_to_remove = ['Issue_id']\n",
    "merged_df.drop(columns=columns_to_remove, inplace=True)\n",
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 5)\n",
      "     Issue ID                                         Issue Body  \\\n",
      "0  2074313481  ### Describe the feature or enhancement\\r\\n\\r\\...   \n",
      "1  2115485810  ### Describe the Bug\\r\\n\\r\\nWe upgraded from 2...   \n",
      "2  2115485810  ### Describe the Bug\\r\\n\\r\\nWe upgraded from 2...   \n",
      "3  2115485810  ### Describe the Bug\\r\\n\\r\\nWe upgraded from 2...   \n",
      "4  2115485810  ### Describe the Bug\\r\\n\\r\\nWe upgraded from 2...   \n",
      "\n",
      "                Type                      Candidate String  \\\n",
      "0  Zipbooks Password                             forgotten   \n",
      "1       Anypoint API  5509fd77-5a93-5372-b0ec-223d33ccbf76   \n",
      "2       Anypoint API  e24894c3-76d1-582a-a1ff-7becec3c8c3d   \n",
      "3       Anypoint API  459a7bb0-e9ed-5cc2-a285-4d1eca9a7fb9   \n",
      "4       Anypoint API  7d66612e-3661-5fc7-aad4-f2b7e8e28bd5   \n",
      "\n",
      "                                       modified_text  \n",
      "0  ### Describe the feature or enhancement\\r\\n\\r\\...  \n",
      "1  c8c96467cb |        41 | www.website.com | ios...  \n",
      "2  ----------+-----------+-----------------+-----...  \n",
      "3  09e269d0ca8 |        27 | www.website.com | io...  \n",
      "4  pageviews |    hostname     |    browser    | ...  \n"
     ]
    }
   ],
   "source": [
    "def create_context_window(text, target_string, window_size=200):\n",
    "\n",
    "    target_index = text.find(target_string)\n",
    "    #print(target_index)\n",
    "\n",
    "    if target_index != -1:\n",
    "        start_index = max(0, target_index - window_size)\n",
    "        end_index = min(len(text), target_index + len(target_string) + window_size)\n",
    "        context_window = text[start_index:end_index]\n",
    "        return context_window\n",
    "\n",
    "    return None\n",
    "\n",
    "# Apply the create_context_window function to each row in the DataFrame\n",
    "merged_df['modified_text'] = merged_df.apply(lambda row: create_context_window(row['Issue Body'], row['Candidate String']), axis=1)\n",
    "print(merged_df.shape)\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch: 0\n",
      "Ok: 23\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "inverse_count=0\n",
    "\n",
    "for i in range(merged_df.shape[0]):\n",
    "  #print(i)\n",
    "  main_string=merged_df['Issue Body'][i]\n",
    "  substring=merged_df['Candidate String'][i]\n",
    "  #print(main_string.find(substring))\n",
    "  if main_string.find(substring)!=-1:\n",
    "    count+=1\n",
    "  else:\n",
    "    inverse_count+=1\n",
    "print(\"Mismatch: \"+str(inverse_count))\n",
    "print(\"Ok: \"+str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_issue_ids = merged_df['Issue ID'].tolist()\n",
    "X_text_test = merged_df['Issue Body'].tolist()  # Convert the 'text' column to a list of strings\n",
    "X_candidate_test = merged_df['Candidate String'].tolist()  # Convert the 'candidate_string' column to a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    return encodings\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text_encodings, candidate_encodings, labels):\n",
    "        self.text_encodings = text_encodings\n",
    "        self.candidate_encodings = candidate_encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx): #it works fine for training\n",
    "        text_input_ids = self.text_encodings['input_ids'][idx]\n",
    "        text_attention_mask = self.text_encodings['attention_mask'][idx]\n",
    "        candidate_input_ids = self.candidate_encodings['input_ids'][idx]\n",
    "        candidate_attention_mask = self.candidate_encodings['attention_mask'][idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "\n",
    "        return text_input_ids, text_attention_mask, candidate_input_ids, candidate_attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/adamW_cntxt200_data25k_pre.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode for inference\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[0;32m   1027\u001b[0m                      map_location,\n\u001b[0;32m   1028\u001b[0m                      pickle_module,\n\u001b[0;32m   1029\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1030\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1443\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\serialization.py:1408\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1407\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1408\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\serialization.py:1382\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1377\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1381\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1382\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1383\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1384\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1387\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\serialization.py:391\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 391\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\serialization.py:266\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 266\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    268\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[1;32mc:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\serialization.py:250\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    247\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    251\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    252\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    253\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    254\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    255\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model_path = \"models/adamW_cntxt200_data25k_pre.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()  # Set the model to evaluation mode for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_body_encodings_test = encode_texts(X_text_test)\n",
    "candidate_encodings_test = encode_texts(X_candidate_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_text_test))\n",
    "Y_labels = [0] * len(X_text_test)\n",
    "Y = np.array(Y_labels)\n",
    "Y_ =Y.astype(int)\n",
    "print(Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(text_body_encodings_test, candidate_encodings_test, Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()  # Set the model in evaluation mode\n",
    "c=0\n",
    "predicted_labels_list = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in test_loader:\n",
    "        print(\"Batch %d\"%c)\n",
    "        c+=1\n",
    "\n",
    "        text_input_ids, text_attention_mask, candidate_input_ids, candidate_attention_mask, labels = batch\n",
    "\n",
    "        # Move tensors to the device\n",
    "        text_input_ids, text_attention_mask, candidate_input_ids, candidate_attention_mask, labels = (\n",
    "            text_input_ids.to(device),\n",
    "            text_attention_mask.to(device),\n",
    "            candidate_input_ids.to(device),\n",
    "            candidate_attention_mask.to(device),\n",
    "            labels.to(device)\n",
    "        )\n",
    "\n",
    "        # Perform inference\n",
    "\n",
    "        outputs = model(input_ids=text_input_ids.type(torch.LongTensor).cuda(), attention_mask=text_attention_mask.type(torch.LongTensor).cuda())\n",
    "        predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        # print(f\"predicted_labels: {predicted_labels}\")\n",
    "        predicted_labels_list.append(predicted_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_list_output = [f.cpu().numpy().tolist() for f in predicted_labels_list]\n",
    "print(predicted_labels_list_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
