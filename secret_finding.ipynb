{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import sklearn.metrics\n",
    "import re\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import hashlib\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate String Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Issue ID                                         Issue Body\n",
      "0  2032018852                   Changed \"ar\" to \"are\" in readme.\n",
      "1  2036967098  ### Describe the Bug\\n\\numami behaving like a ...\n",
      "2  2039165352  Closes #800, #2423, #2400, #2386, #2383, #2358...\n",
      "3  2041048669  ### Describe the Bug\\n\\nImpossible to load dat...\n",
      "4  2048399912  ### Describe the feature or enhancement\\n\\nWe ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "df = pd.read_csv(\"crawled_issue/output.csv\")\n",
    "df_unique = df.drop_duplicates(subset='Issue ID', keep='first')\n",
    "df = df_unique\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install openpyxl \n",
    "excel_data = pd.read_excel('dataset/Secret-Regular-Expression.xlsx')\n",
    "\n",
    "# Read the values of the file in the dataframe\n",
    "regex = pd.DataFrame(excel_data, columns=[\n",
    "'Pattern_ID','Secret Type',\t'Regular Expression','Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue ID</th>\n",
       "      <th>Issue Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2032018852</td>\n",
       "      <td>Changed ar to are in readme.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2036967098</td>\n",
       "      <td>### Describe the Bug\\n\\numami behaving like a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2039165352</td>\n",
       "      <td>Closes #800, #2423, #2400, #2386, #2383, #2358...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2041048669</td>\n",
       "      <td>### Describe the Bug\\n\\nImpossible to load dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2048399912</td>\n",
       "      <td>### Describe the feature or enhancement\\n\\nWe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2053026441</td>\n",
       "      <td>fixed #2433  bug please review\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2053308325</td>\n",
       "      <td>This PR includes updates to the Swedish transl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2054244873</td>\n",
       "      <td>### Describe the Bug\\n\\n1. Start onboarding, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2056044123</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\ni want to track re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2058073803</td>\n",
       "      <td>This PR corrects a spelling error in the Share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2064913595</td>\n",
       "      <td>### Describe the Bug\\n\\nInside of the website ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2066413072</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\nAbout three weeks ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2067476483</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\nI have both [Verce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2069253235</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\nUsing Umami Cloud ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2072231190</td>\n",
       "      <td>### Describe the Bug\\n\\n![image](\\r\\n\\r\\nBeen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2074313481</td>\n",
       "      <td>### Describe the feature or enhancement\\n\\nHow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2075188399</td>\n",
       "      <td>### Describe the Bug\\n\\nHello everyone,\\r\\n\\r\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2075567089</td>\n",
       "      <td>Bumps [follow-redirects]( from  to .\\n\\nCommit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2082537025</td>\n",
       "      <td>fix pgbouncer to supavisor migration issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2083725701</td>\n",
       "      <td>This pull requests implements logging of the I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2091740149</td>\n",
       "      <td>### Describe the Bug\\n\\nHi! Im attempting to m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2092019139</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\nWhen running Umami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2093753035</td>\n",
       "      <td>### Describe the Bug\\n\\nI have been mighty bus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2107214385</td>\n",
       "      <td>### Describe the Bug\\n\\nthe documentation ment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2115485810</td>\n",
       "      <td>### Describe the Bug\\r\\n\\r\\nWe upgraded from  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2116188130</td>\n",
       "      <td>### Describe the Bug\\n\\n![image](\\r\\n![image](...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2117280225</td>\n",
       "      <td>### Describe the Bug\\n\\nWhen logging in using ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Issue ID                                         Issue Body\n",
       "0   2032018852                       Changed ar to are in readme.\n",
       "1   2036967098  ### Describe the Bug\\n\\numami behaving like a ...\n",
       "2   2039165352  Closes #800, #2423, #2400, #2386, #2383, #2358...\n",
       "3   2041048669  ### Describe the Bug\\n\\nImpossible to load dat...\n",
       "4   2048399912  ### Describe the feature or enhancement\\n\\nWe ...\n",
       "5   2053026441                 fixed #2433  bug please review\\r\\n\n",
       "6   2053308325  This PR includes updates to the Swedish transl...\n",
       "7   2054244873  ### Describe the Bug\\n\\n1. Start onboarding, c...\n",
       "8   2056044123  ### Describe the Bug\\r\\n\\r\\ni want to track re...\n",
       "9   2058073803  This PR corrects a spelling error in the Share...\n",
       "10  2064913595  ### Describe the Bug\\n\\nInside of the website ...\n",
       "11  2066413072  ### Describe the Bug\\r\\n\\r\\nAbout three weeks ...\n",
       "12  2067476483  ### Describe the Bug\\r\\n\\r\\nI have both [Verce...\n",
       "13  2069253235  ### Describe the Bug\\r\\n\\r\\nUsing Umami Cloud ...\n",
       "14  2072231190  ### Describe the Bug\\n\\n![image](\\r\\n\\r\\nBeen ...\n",
       "15  2074313481  ### Describe the feature or enhancement\\n\\nHow...\n",
       "16  2075188399  ### Describe the Bug\\n\\nHello everyone,\\r\\n\\r\\...\n",
       "17  2075567089  Bumps [follow-redirects]( from  to .\\n\\nCommit...\n",
       "18  2082537025         fix pgbouncer to supavisor migration issue\n",
       "19  2083725701  This pull requests implements logging of the I...\n",
       "20  2091740149  ### Describe the Bug\\n\\nHi! Im attempting to m...\n",
       "21  2092019139  ### Describe the Bug\\r\\n\\r\\nWhen running Umami...\n",
       "22  2093753035  ### Describe the Bug\\n\\nI have been mighty bus...\n",
       "23  2107214385  ### Describe the Bug\\n\\nthe documentation ment...\n",
       "24  2115485810  ### Describe the Bug\\r\\n\\r\\nWe upgraded from  ...\n",
       "25  2116188130  ### Describe the Bug\\n\\n![image](\\r\\n![image](...\n",
       "26  2117280225  ### Describe the Bug\\n\\nWhen logging in using ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_dict={}\n",
    "for j in df.index:\n",
    "        # if df[\"id\"][j] != \"1165939311\":\n",
    "        #         continue\n",
    "        input_string =    str(df[\"Issue Body\"][j])    \n",
    "        input_string = re.sub(r'[\\'\"\\â”‚]', '', input_string)\n",
    "        dir_list_clean = re.sub(r'drwx[-\\s]*\\d+\\s+\\w+\\s+\\w+\\s+\\d+\\s+\\w+\\s+\\d+\\s+[0-9a-fA-F-]+.*','',input_string)\n",
    "        shell_code_free_text = re.sub(r'```shell([^`]+)```','',dir_list_clean,flags=re.IGNORECASE)\n",
    "        shell_code_free_text = re.sub(r'```Shell\\s*\"([^\"]*)\"\\s*```','',shell_code_free_text,flags=re.IGNORECASE)\n",
    "        # saved_game_free_text = re.sub(r'```([^`]+)```','',shell_code_free_text) #etay jhamela hobe\n",
    "        saved_game_free_text = re.sub(r'<details><summary>Saved game</summary>\\n\\n```(.*?)```', '', shell_code_free_text)\n",
    "        remove_packages = re.sub(r'(\\w+\\.)+\\w+','',saved_game_free_text)\n",
    "        java_exp_free_text = re.sub(r'at\\s[\\w.$]+\\.([\\w]+)\\(([^:]+:\\d+)\\)','',remove_packages)\n",
    "        # url_free_text= re.sub(https?://[^\\s#]+#[A-Za-z0-9\\-]+,'', java_exp_free_text, flags=re.IGNORECASE)\n",
    "        url_with_fragment_text= re.sub(r'https?://[^\\s#]+#[A-Za-z0-9\\-\\=\\+]+','', java_exp_free_text, flags=re.IGNORECASE)\n",
    "        url_free_text= re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '',url_with_fragment_text)\n",
    "        commit_free_text= re.sub(r'commit[ ]?(?:id)?[ ]?[:]?[ ]?([0-9a-f]{40})\\b', '', url_free_text, flags=re.IGNORECASE)\n",
    "        file_path_free_text = re.sub(r\"/[\\w/. :-]+\",'',commit_free_text)\n",
    "        file_path_free_text = re.sub( r'(/[^/\\s]+)+','',file_path_free_text)\n",
    "        sha256_free_text = re.sub(r'sha256\\s*[:]?[=]?\\s*[a-fA-F0-9]{64}','',file_path_free_text)\n",
    "        sha1_free_text = re.sub(r'git-tree-sha1\\s*=\\s*[a-fA-F0-9]+','',sha256_free_text)\n",
    "        build_id_free_text = re.sub(r'build-id\\s*[:]?[=]?\\s*([a-fA-F0-9]+)','',sha1_free_text)\n",
    "        guids_free_text = re.sub(r'GUIDs:\\s+([0-9a-fA-F-]+\\s+[0-9a-fA-F-]+\\s+[0-9a-fA-F-]+)','',build_id_free_text)\n",
    "        uuids_free_text = re.sub(r'([0-9a-fA-F-]+\\s*,\\s*[0-9a-fA-F-]+\\s*,\\s*[0-9a-fA-F-]+)','',guids_free_text)\n",
    "        event_id_free_text = re.sub(r'<([^>]+)>','',uuids_free_text)\n",
    "        UUID_free_text = re.sub(r'(?:UUID|GUID|version|id)[\\\\=:\"\\'\\s]*\\b[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}\\b'\n",
    ",'',event_id_free_text,flags=re.IGNORECASE) ##without the prefix so many false positives can be omitted\n",
    "        hex_free_text = re.sub(r'(?:data|address|id)[\\\\=:\"\\'\\s]*\\b0x[0-9a-fA-F]+\\b','',UUID_free_text,flags=re.IGNORECASE) ## deleting hex ids directly can cause issues\n",
    "        ss_free_text = re.sub(r'Screenshot_(\\d{4}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\d{2}[_-]\\w+)','',hex_free_text,flags=re.IGNORECASE)\n",
    "        cleaned_text = ss_free_text\n",
    "        # file_path = \"output.txt\"\n",
    "\n",
    "        # with open(file_path, 'w') as file:\n",
    "        #                 file.write(cleaned_text)\n",
    "        data_dict[j] = {'Issue ID':df['Issue ID'][j],'Issue Body':cleaned_text}\n",
    "        # idx = idx+1\n",
    "    \n",
    "\n",
    "\n",
    "cleaned_text_data = pd.DataFrame.from_dict(data_dict, \"index\")\n",
    "cleaned_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_17968\\1642664489.py:11: FutureWarning: Possible nested set at position 35\n",
      "  p = re.compile(regex['Regular Expression'][i])\n",
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_17968\\1642664489.py:11: DeprecationWarning: Flags not at the start of the expression '/(?i)-----\\\\s*?BEGIN[' (truncated) but at position 1\n",
      "  p = re.compile(regex['Regular Expression'][i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 3)\n"
     ]
    }
   ],
   "source": [
    "###recovered\n",
    "    \n",
    "idx = 0\n",
    "data_dict={}\n",
    "# start = iter*100000\n",
    "# end = (iter+1)*100000\n",
    "for i in regex.index:\n",
    "    #print(i,regex['Secret Type'][i]) #, regex['Regular Expression'][i])\n",
    "    # if i%100==0:\n",
    "    #     print(\"checkpoint\")\n",
    "    p = re.compile(regex['Regular Expression'][i])\n",
    "    \n",
    "    # print(\"=====================================================================\")\n",
    "    \n",
    "    for j in df.index:\n",
    "        \n",
    "        cleaned_text = cleaned_text_data.loc[j, 'Issue Body']\n",
    "            # Now you can use 'cleaned_text' for further processing\n",
    "       \n",
    "        matches = re.findall(p,cleaned_text)\n",
    "        for match in set(matches):\n",
    "                data_dict[idx] = {'Type': regex['Secret Type'][i], 'Issue ID':df['Issue ID'][j],'Candidate String':match} #,'Entropy':shannon_entropy(match)}\n",
    "                idx = idx+1\n",
    "    \n",
    "\n",
    "\n",
    "data = pd.DataFrame.from_dict(data_dict, \"index\")\n",
    "data=data.drop_duplicates(subset=[\"Issue ID\", \"Candidate String\"], keep='first')\n",
    "print(data.shape)\n",
    "data.to_csv('crawled_issue/issues-with-candidate-strings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context Window and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 3)\n",
      "           Type    Issue_id                      Candidate String\n",
      "0  Anypoint API  2115485810  5509fd77-5a93-5372-b0ec-223d33ccbf76\n",
      "1  Anypoint API  2115485810  9a3ae813-616d-5ea0-a782-35d76892bf8d\n",
      "2  Anypoint API  2115485810  bc070bf5-d5da-5147-8222-1e520817f284\n",
      "3  Anypoint API  2115485810  78748dc1-88d8-59d2-8262-329895cc000c\n",
      "4  Anypoint API  2115485810  c90eab1e-ef35-5f90-9c00-3e019a5605c1\n",
      "(22, 5)\n",
      "Index(['Issue ID', 'Issue Body', 'Type', 'Candidate String'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = data.rename(columns={'Issue ID': 'Issue_id'})\n",
    "print(data.shape)\n",
    "print(data.head())\n",
    "merged_df = df.merge(data, left_on='Issue ID', right_on='Issue_id')\n",
    "print(merged_df.shape)\n",
    "columns_to_remove = ['Issue_id']\n",
    "merged_df.drop(columns=columns_to_remove, inplace=True)\n",
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 5)\n",
      "     Issue ID                                         Issue Body  \\\n",
      "0  2074313481  ### Describe the feature or enhancement\\n\\nHow...   \n",
      "1  2115485810  ### Describe the Bug\\r\\n\\r\\nWe upgraded from 2...   \n",
      "2  2115485810  ### Describe the Bug\\r\\n\\r\\nWe upgraded from 2...   \n",
      "3  2115485810  ### Describe the Bug\\r\\n\\r\\nWe upgraded from 2...   \n",
      "4  2115485810  ### Describe the Bug\\r\\n\\r\\nWe upgraded from 2...   \n",
      "\n",
      "                Type                      Candidate String  \\\n",
      "0  Zipbooks Password                             forgotten   \n",
      "1       Anypoint API  5509fd77-5a93-5372-b0ec-223d33ccbf76   \n",
      "2       Anypoint API  9a3ae813-616d-5ea0-a782-35d76892bf8d   \n",
      "3       Anypoint API  bc070bf5-d5da-5147-8222-1e520817f284   \n",
      "4       Anypoint API  78748dc1-88d8-59d2-8262-329895cc000c   \n",
      "\n",
      "                                       modified_text  \n",
      "0  ### Describe the feature or enhancement\\n\\nHow...  \n",
      "1  c8c96467cb |        41 | www.website.com | ios...  \n",
      "2  ----------+-----------------+---------------+-...  \n",
      "3  96face7eee83 |        18 | www.website.com | i...  \n",
      "4  |       615 | www.website.com | chrome        ...  \n"
     ]
    }
   ],
   "source": [
    "def create_context_window(text, target_string, window_size=200):\n",
    "\n",
    "    target_index = text.find(target_string)\n",
    "    #print(target_index)\n",
    "\n",
    "    if target_index != -1:\n",
    "        start_index = max(0, target_index - window_size)\n",
    "        end_index = min(len(text), target_index + len(target_string) + window_size)\n",
    "        context_window = text[start_index:end_index]\n",
    "        return context_window\n",
    "\n",
    "    return None\n",
    "\n",
    "# Apply the create_context_window function to each row in the DataFrame\n",
    "merged_df['modified_text'] = merged_df.apply(lambda row: create_context_window(row['Issue Body'], row['Candidate String']), axis=1)\n",
    "print(merged_df.shape)\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch: 0\n",
      "Ok: 22\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "inverse_count=0\n",
    "\n",
    "for i in range(merged_df.shape[0]):\n",
    "  #print(i)\n",
    "  main_string=merged_df['Issue Body'][i]\n",
    "  substring=merged_df['Candidate String'][i]\n",
    "  #print(main_string.find(substring))\n",
    "  if main_string.find(substring)!=-1:\n",
    "    count+=1\n",
    "  else:\n",
    "    inverse_count+=1\n",
    "print(\"Mismatch: \"+str(inverse_count))\n",
    "print(\"Ok: \"+str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahmed\\miniconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_issue_ids = merged_df['Issue ID'].tolist()\n",
    "X_text_test = merged_df['Issue Body'].tolist()  # Convert the 'text' column to a list of strings\n",
    "X_candidate_test = merged_df['Candidate String'].tolist()  # Convert the 'candidate_string' column to a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    return encodings\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text_encodings, candidate_encodings, labels):\n",
    "        self.text_encodings = text_encodings\n",
    "        self.candidate_encodings = candidate_encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx): #it works fine for training\n",
    "        text_input_ids = self.text_encodings['input_ids'][idx]\n",
    "        text_attention_mask = self.text_encodings['attention_mask'][idx]\n",
    "        candidate_input_ids = self.candidate_encodings['input_ids'][idx]\n",
    "        candidate_attention_mask = self.candidate_encodings['attention_mask'][idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "\n",
    "        return text_input_ids, text_attention_mask, candidate_input_ids, candidate_attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model_path = \"models/local_roberta_model_10epoch_lr1e-5_torchrmsprop_cntxt200_data25k.pth\"\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)  # Replace with your model configuration\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()  # Set the model to evaluation mode for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_body_encodings_test = encode_texts(X_text_test)\n",
    "candidate_encodings_test = encode_texts(X_candidate_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(len(X_text_test))\n",
    "Y_labels = [0] * len(X_text_test)\n",
    "Y = np.array(Y_labels)\n",
    "Y_ =Y.astype(int)\n",
    "print(Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(text_body_encodings_test, candidate_encodings_test, Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n",
      "Batch 4\n",
      "Batch 5\n",
      "Batch 6\n",
      "Batch 7\n",
      "Batch 8\n",
      "Batch 9\n",
      "Batch 10\n",
      "Batch 11\n",
      "Batch 12\n",
      "Batch 13\n",
      "Batch 14\n",
      "Batch 15\n",
      "Batch 16\n",
      "Batch 17\n",
      "Batch 18\n",
      "Batch 19\n",
      "Batch 20\n",
      "Batch 21\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()  # Set the model in evaluation mode\n",
    "c=0\n",
    "predicted_labels_list = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in test_loader:\n",
    "        print(\"Batch %d\"%c)\n",
    "        c+=1\n",
    "\n",
    "        text_input_ids, text_attention_mask, candidate_input_ids, candidate_attention_mask, labels = batch\n",
    "\n",
    "        # Move tensors to the device\n",
    "        text_input_ids, text_attention_mask, candidate_input_ids, candidate_attention_mask, labels = (\n",
    "            text_input_ids.to(device),\n",
    "            text_attention_mask.to(device),\n",
    "            candidate_input_ids.to(device),\n",
    "            candidate_attention_mask.to(device),\n",
    "            labels.to(device)\n",
    "        )\n",
    "\n",
    "        # Perform inference\n",
    "\n",
    "        outputs = model(input_ids=text_input_ids.type(torch.LongTensor).cuda(), attention_mask=text_attention_mask.type(torch.LongTensor).cuda())\n",
    "        predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        # print(f\"predicted_labels: {predicted_labels}\")\n",
    "        predicted_labels_list.append(predicted_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_list_output = [f.cpu().numpy().tolist() for f in predicted_labels_list]\n",
    "print(predicted_labels_list_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
